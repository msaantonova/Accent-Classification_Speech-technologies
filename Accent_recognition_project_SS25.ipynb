{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msaantonova/Accent-Classification_Speech-technologies/blob/main/Accent_recognition_project_SS25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "utPRj8QSfBW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repository from the paper\n",
        "\n",
        "\"This repository provides all the necessary tools to perform accent identification from speech recordings with SpeechBrain toolkit! The system uses a model fine-tuned on the CommonAccent dataset in English (21 accents). The provided system can recognize the following 21 accents of English from short speech recordings\""
      ],
      "metadata": {
        "id": "6uOohfJNCNLx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvKoxtf7-l7s",
        "outputId": "cd71297b-3a31-4b7e-e839-dfe403339e7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'accent-recog-slt2022'...\n",
            "remote: Enumerating objects: 527, done.\u001b[K\n",
            "remote: Counting objects: 100% (204/204), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 527 (delta 182), reused 155 (delta 154), pack-reused 323 (from 1)\u001b[K\n",
            "Receiving objects: 100% (527/527), 2.64 MiB | 5.22 MiB/s, done.\n",
            "Resolving deltas: 100% (342/342), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/lgspeech/accent-recog-slt2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAG2EUzBeABn",
        "outputId": "6b029e1c-1f04-48f0-b8bc-f9e9f7fdaeb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/accent-recog-slt2022\n",
            "Ignoring SoundFile: markers 'sys_platform == \"win32\"' don't match your environment\n",
            "Collecting speechbrain==0.5.13 (from -r requirements.txt (line 1))\n",
            "  Downloading speechbrain-0.5.13-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting datasets==2.8.0 (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-2.8.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting transformers==4.25.1 (from -r requirements.txt (line 3))\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl.metadata (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting librosa==0.9.2 (from -r requirements.txt (line 4))\n",
            "  Downloading librosa-0.9.2-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting ipdb>=0.13.9 (from -r requirements.txt (line 5))\n",
            "  Downloading ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.2.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.33.0)\n",
            "Collecting hyperpyyaml>=0.0.1 (from -r requirements.txt (line 8))\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (1.5.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (24.2)\n",
            "Collecting pre-commit>=2.3.0 (from -r requirements.txt (line 12))\n",
            "  Downloading pre_commit-4.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting scipy<1.9,>=1.4.1 (from -r requirements.txt (line 13))\n",
            "  Downloading scipy-1.6.1.tar.gz (27.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.3/27.3 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (2.6.0+cu124)\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 2.0.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchaudio<=0.11.0,>=0.9.0 (from versions: 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchaudio<=0.11.0,>=0.9.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%cd /content/accent-recog-slt2022\n",
        "!python -m pip install -r requirements.txt\n",
        "# you can ignore any errors for now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iQrLs6ilfGon",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca77fbb2-29ac-48f4-ef4c-698fdffd892b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/474.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m450.6/474.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/177.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.6.1 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets==3.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change working directory:\n",
        "%cd CommonAccent/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m6-V5CHYS-V",
        "outputId": "ee19fc47-74dd-43eb-e28a-e360b6c8e8a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/accent-recog-slt2022/CommonAccent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the data (or stream) and create the list of filtered items\n",
        "\n",
        "You can either use streaming, or save the files locally. you can check at common voice, how big the corpus for each language is.\n",
        "If you use streaming, it  will take a few hours.\n",
        "\n",
        "The aim of it is to create lists of all audios that have marked accent in the metadata, so you can choose which ones of these you want for classification."
      ],
      "metadata": {
        "id": "gAQzxiFMYUnj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAt-X0eXezdf",
        "outputId": "692d63c8-5371-43ff-841e-c3dee49fc101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/download_data_hf.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "#this part descrbes the dataset index and the language\n",
        "!python download_data_hf.py --language \"it\" data/cv_11/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the filtered list to be used in this model --> create csv files in correct form"
      ],
      "metadata": {
        "id": "xDqntrpPYjUn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Zgu6_R0J6dP_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "265510a5-72a1-455e-cfbd-946825821583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.0/499.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/363.4 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute '_str'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3796896608>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -q speechbrain==0.5.13      transformers==4.25.1      librosa==0.9.2      ipdb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     return {\n\u001b[1;32m   1086\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     }\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    605\u001b[0m             make_files(\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \"\"\"\n\u001b[1;32m   1234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \"\"\"\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m__fspath__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mas_posix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             self._str = self._format_parsed_parts(self._drv, self._root,\n\u001b[0m\u001b[1;32m    543\u001b[0m                                                   self._parts) or '.'\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m_format_parsed_parts\u001b[0;34m(cls, drv, root, parts)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_format_parsed_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdrv\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!pip install -q speechbrain==0.5.13 \\\n",
        "    transformers==4.25.1 \\\n",
        "    librosa==0.9.2 \\\n",
        "    ipdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python common_accent_prepare.py --language \"it\" data/cv_11 data/"
      ],
      "metadata": {
        "id": "TJ25UXrSYQpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import write\n",
        "from datasets import load_dataset\n",
        "\n",
        "data_path = \"/content/accent-recog-slt2022/CommonAccent/data/\"\n",
        "# Load CSV\n",
        "csv_path_train = data_path + \"train.csv\"\n",
        "df_train = pd.read_csv(csv_path_train)\n",
        "file_list_train = list(set(df_train['wav']))\n",
        "\n",
        "csv_path_test = data_path + \"test.csv\"\n",
        "df_test = pd.read_csv(csv_path_test)\n",
        "file_list_test = list(set(df_test['wav']))\n",
        "\n",
        "csv_path_dev = data_path + \"dev.csv\"\n",
        "df_dev = pd.read_csv(csv_path_dev)\n",
        "file_list_dev = list(set(df_dev['wav']))\n",
        "\n",
        "# Set save directory\n",
        "save_dir = data_path + \"wav_files\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Load dataset in streaming mode\n",
        "language = \"it\"  # or whatever language code you're using\n",
        "cv_19_train = load_dataset(\"fsicoli/common_voice_19_0\", language, streaming=True, split=\"train\", trust_remote_code=True)\n",
        "print(next(iter(cv_19_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pHghJSvEuEq",
        "outputId": "6ad6a31e-3f22-402f-fd92-e8410005b469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Reading metadata...: 171388it [00:07, 23662.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'client_id': 'c14f21cacf2b7701ad0bead0dd1b31ec9d3a1557708e446de55e98b4b470cf31072c82543e5ba518c5c187a91868878a4e32727054a3dd94f9df41c9a13d8c62', 'path': 'it_train_0/common_voice_it_17415777.mp3', 'audio': {'path': 'it_train_0/common_voice_it_17415777.mp3', 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
            "       -1.61342832e-05, -1.57291943e-05, -1.16234423e-05]), 'sampling_rate': 48000}, 'sentence': \"Il marchese aveva già moglie in quell'epoca?\", 'up_votes': 3, 'down_votes': 0, 'age': '', 'gender': '', 'accent': '', 'locale': 'it', 'segment': '', 'variant': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_19_dev = load_dataset(\"fsicoli/common_voice_19_0\", language, streaming=False, split=\"validation\", trust_remote_code=True)\n",
        "cv_19_test = load_dataset(\"fsicoli/common_voice_19_0\", language, streaming=False, split=\"test\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "PMZFhNN_V2Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Count entries per accent\n",
        "accent_counts = df_train['accent'].value_counts()\n",
        "\n",
        "# Print the result\n",
        "print(accent_counts)\n",
        "\n",
        "# select randomly max 300 samples for each accent that has more than 300 samples\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Uyu3fx-SAlo",
        "outputId": "003c3702-9ce5-40c1-8950-9693771089f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accent\n",
            "TENDENTE AL SICULO MA NON MARCATO    1801\n",
            "BASILICATA TRENTINO                  1801\n",
            "VENETO                               1393\n",
            "MERIDIONALE                           134\n",
            "EMILIANO                               91\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This code adjusts the audio paths and can be used for balancing the data:"
      ],
      "metadata": {
        "id": "1bk_7sQqCp-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# add /content/accent-recog-slt2022/CommonAccent/data_small to each \"wav\" in the csv .../train.csv\n",
        "save_dir_colab = data_path\n",
        "# \"content/accent-recog-slt2022/CommonAccent/data_small/\"\n",
        "df_train['wav'] = df_train['wav'].apply(lambda x: os.path.join(save_dir_colab, x))\n",
        "df_test['wav'] = df_test['wav'].apply(lambda x: os.path.join(save_dir_colab, x))\n",
        "df_dev['wav'] = df_dev['wav'].apply(lambda x: os.path.join(save_dir_colab, x))\n",
        "df_train.to_csv(csv_path_train, index=False)\n",
        "\n",
        "# show all available accents\n",
        "accent_counts = df_train['accent'].value_counts()\n",
        "\n",
        "# Print the result\n",
        "print(accent_counts)\n",
        "\n",
        "# select randomly max 300 samples for each accent\n",
        "df_train2 = df_train.groupby('accent').apply(lambda x: x.sample(min(len(x), 300), random_state=1)).reset_index(drop=True)\n",
        "df_dev2 = df_dev.groupby('accent').apply(lambda x: x.sample(min(len(x), 50), random_state=1)).reset_index(drop=True)\n",
        "df_test2 = df_test.groupby('accent').apply(lambda x: x.sample(min(len(x), 50), random_state=1)).reset_index(drop=True)\n",
        "# remove accents with less than 300 samples\n",
        "df_train2 = df_train2.groupby('accent').filter(lambda x: len(x) >= 300)\n",
        "# get names of remaining accents\n",
        "remaining_accents = df_train2['accent'].unique()\n",
        "# filter dev and test sets to keep only the remaining accents\n",
        "df_dev2 = df_dev2[df_dev2['accent'].isin(remaining_accents)]\n",
        "df_test2 = df_test2[df_test2['accent'].isin(remaining_accents)]\n",
        "# reduce to 50 samples for dev and test sets\n",
        "\n",
        "# print stats of the new dataframe\n",
        "accent_counts2_train = df_train2['accent'].value_counts()\n",
        "accent_counts2_dev = df_dev2['accent'].value_counts()\n",
        "accent_counts2_test = df_test2['accent'].value_counts()\n",
        "# Print the result\n",
        "print(accent_counts2_train)\n",
        "print(accent_counts2_dev)\n",
        "print(accent_counts2_test)\n",
        "\n",
        "# save csv again with these samples\n",
        "os.makedirs('/content/accent-recog-slt2022/CommonAccent/data_small', exist_ok=True)\n",
        "df_train2.to_csv('/content/accent-recog-slt2022/CommonAccent/data_small/train.csv', index=False)\n",
        "df_dev2.to_csv('/content/accent-recog-slt2022/CommonAccent/data_small/dev.csv', index=False)\n",
        "df_test2.to_csv('/content/accent-recog-slt2022/CommonAccent/data_small/test.csv', index=False)\n",
        "# download files to disk\n",
        "!zip -r /content/accent-recog-slt2022/CommonAccent/data_small.zip /content/accent-recog-slt2022/CommonAccent/data_small\n",
        "# download\n",
        "from google.colab import files\n",
        "files.download(\"/content/accent-recog-slt2022/CommonAccent/data_small.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-PmRkAJCnmV",
        "outputId": "dd9d16e7-0c9d-4044-ba9c-7e05c7b4a731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accent\n",
            "TENDENTE AL SICULO MA NON MARCATO    1801\n",
            "BASILICATA TRENTINO                  1801\n",
            "VENETO                               1393\n",
            "MERIDIONALE                           134\n",
            "EMILIANO                               91\n",
            "Name: count, dtype: int64\n",
            "accent\n",
            "BASILICATA TRENTINO                  300\n",
            "TENDENTE AL SICULO MA NON MARCATO    300\n",
            "VENETO                               300\n",
            "Name: count, dtype: int64\n",
            "accent\n",
            "BASILICATA TRENTINO                  50\n",
            "TENDENTE AL SICULO MA NON MARCATO    50\n",
            "VENETO                               50\n",
            "Name: count, dtype: int64\n",
            "accent\n",
            "BASILICATA TRENTINO                  50\n",
            "TENDENTE AL SICULO MA NON MARCATO    50\n",
            "VENETO                               50\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-f3c15a7b6ddf>:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df_train2 = df_train.groupby('accent').apply(lambda x: x.sample(min(len(x), 300), random_state=1)).reset_index(drop=True)\n",
            "<ipython-input-20-f3c15a7b6ddf>:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df_dev2 = df_dev.groupby('accent').apply(lambda x: x.sample(min(len(x), 50), random_state=1)).reset_index(drop=True)\n",
            "<ipython-input-20-f3c15a7b6ddf>:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df_test2 = df_test.groupby('accent').apply(lambda x: x.sample(min(len(x), 50), random_state=1)).reset_index(drop=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save audios locally (takes around 27min for italian)\n",
        "for item1 in cv_19_train:\n",
        "    # Check if the current file path is in the CSV file list\n",
        "    # print(item['path'])\n",
        "    # adjust the following line to your working directory\n",
        "    filename = (item1['path'].split('/')[-2] + '/' + item1['path'].split('/')[-1])\n",
        "    if filename in file_list_train or filename in file_list_test or filename in file_list_dev:\n",
        "      # Decode audio\n",
        "      audio_array = item1['audio']['array']  # Audio waveform\n",
        "      sampling_rate = item1['audio']['sampling_rate']  # Sampling rate\n",
        "\n",
        "      # Create the filename\n",
        "      file_name = os.path.join(save_dir, filename)\n",
        "      os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "\n",
        "      # Save the audio as a WAV file\n",
        "      write(file_name, sampling_rate, audio_array.astype(np.float32))  # Save as float32 for WAV compatibility\n"
      ],
      "metadata": {
        "id": "qYFe1THtuHEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takes 2 min for it dev, test\n",
        "for item1 in cv_19_dev:\n",
        "    filename = (item1['path'].split('/')[-2] + '/' + item1['path'].split('/')[-1])\n",
        "    if filename in file_list_train or filename in file_list_test or filename in file_list_dev:\n",
        "      # Decode audio\n",
        "      audio_array = item1['audio']['array']  # Audio waveform\n",
        "      sampling_rate = item1['audio']['sampling_rate']  # Sampling rate\n",
        "\n",
        "      # Create the filename\n",
        "      file_name = os.path.join(save_dir, filename)\n",
        "      os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "\n",
        "      # Save the audio as a WAV file\n",
        "      write(file_name, sampling_rate, audio_array.astype(np.float32))  # Save as float32 for WAV compatibility"
      ],
      "metadata": {
        "id": "VtBeEGDtYCRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item1 in cv_19_test:\n",
        "    filename = (item1['path'].split('/')[-2] + '/' + item1['path'].split('/')[-1])\n",
        "    if filename in file_list_train or filename in file_list_test or filename in file_list_dev:\n",
        "      # Decode audio\n",
        "      audio_array = item1['audio']['array']  # Audio waveform\n",
        "      sampling_rate = item1['audio']['sampling_rate']  # Sampling rate\n",
        "\n",
        "      # Create the filename\n",
        "      file_name = os.path.join(save_dir, filename)\n",
        "      os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "\n",
        "      # Save the audio as a WAV file\n",
        "      write(file_name, sampling_rate, audio_array.astype(np.float32))  # Save as float32 for WAV compatibility"
      ],
      "metadata": {
        "id": "ZfyNah9KYIYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this step I would suggest to copy the .csv files and also the wav files to your gdrive so that you can load them from there next time.\n",
        "Next time (if you saved the wav files in your gdrive) you can copy it back to colab and can continue from there."
      ],
      "metadata": {
        "id": "-1n3cTWFZUVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir -p /content/accent-recog-slt2022/CommonAccent/data_small\n",
        "# !cp -R /content/drive/MyDrive/.../data/wav /content/accent-recog-slt2022/CommonAccent/data_small/wav_files"
      ],
      "metadata": {
        "id": "VGFp_4Bf_d_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -R /content/accent-recog-slt2022/CommonAccent/data_small/wav_files /content/drive/MyDrive/.../data/wav"
      ],
      "metadata": {
        "id": "IF5PqPbiXsdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "import librosa\n",
        "import os\n",
        "\n",
        "# resample all files to 16khz\n",
        "base_dir = '/content/accent-recog-slt2022/CommonAccent/data_small/wav_files/wav_files/'\n",
        "\n",
        "# Walk through all subdirectories\n",
        "for root, dirs, files in os.walk(base_dir):\n",
        "    for filename in files:\n",
        "        if filename.endswith('.mp3'):\n",
        "            audio_file = os.path.join(root, filename)\n",
        "            try:\n",
        "                # Load with librosa to ensure mono and 16kHz resampling\n",
        "                data, _ = librosa.load(audio_file, sr=16000, mono=True)\n",
        "                # Output path with .wav extension\n",
        "                new_path = audio_file.replace('.mp3', '.wav')\n",
        "                # Save as 16kHz mono .wav\n",
        "                sf.write(new_path, data, 16000)\n",
        "                # print(f\"Converted and saved: {new_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {audio_file}: {e}\")\n"
      ],
      "metadata": {
        "id": "2juSor7lxzis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies for training the model\n",
        "You should not get any errors here"
      ],
      "metadata": {
        "id": "fXxkRQoeD0bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pandas==2.2.2 \\\n",
        "    huggingface_hub>=0.7.0 \\\n",
        "    hyperpyyaml==0.0.1 \\\n",
        "    joblib \\\n",
        "    numpy==2.0.2 \\\n",
        "    packaging \\\n",
        "    pre-commit==2.3.0 \\\n",
        "    sentencepiece>=0.1.91 \\\n",
        "    SoundFile \\\n",
        "    tqdm"
      ],
      "metadata": {
        "id": "1oTznOs9vSkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga3ThIIw8q5K"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You will probably have to adjust your audiopaths in the csv files:"
      ],
      "metadata": {
        "id": "p_NN_MHhEAyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/accent-recog-slt2022/CommonAccent/data_small/train.csv /content/accent-recog-slt2022/CommonAccent/data_small/train_orig.csv\n",
        "!cp /content/accent-recog-slt2022/CommonAccent/data_small/dev.csv /content/accent-recog-slt2022/CommonAccent/data_small/dev_orig.csv\n",
        "!cp /content/accent-recog-slt2022/CommonAccent/data_small/test.csv /content/accent-recog-slt2022/CommonAccent/data_small/test_orig.csv"
      ],
      "metadata": {
        "id": "GwlLLx15DxV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "def update_csv_with_full_path_and_duration(\n",
        "    input_csv_path,\n",
        "    output_csv_path,\n",
        "    base_audio_path\n",
        "):\n",
        "    modified_rows = []\n",
        "\n",
        "    with open(input_csv_path, mode='r', newline='', encoding='utf-8') as infile:\n",
        "        reader = csv.DictReader(infile)\n",
        "\n",
        "        for row in reader:\n",
        "            try:\n",
        "                audio_path = os.path.join(base_audio_path, row['wav'].replace('.mp3', '.wav'))\n",
        "\n",
        "                data, sampling_rate = sf.read(audio_path)\n",
        "                duration = np.round(librosa.get_duration(y=data, sr=sampling_rate), 3)\n",
        "\n",
        "                row['duration'] = duration\n",
        "                row['wav'] = audio_path\n",
        "\n",
        "                modified_rows.append(row)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed processing {row['wav']}: {e}\")\n",
        "\n",
        "        fieldnames = reader.fieldnames\n",
        "\n",
        "    with open(output_csv_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(modified_rows)\n"
      ],
      "metadata": {
        "id": "PEaM7tT-NCRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_audio_path = '/content/accent-recog-slt2022/CommonAccent/data_small/wav_files/wav_files/'\n",
        "\n",
        "# Train\n",
        "update_csv_with_full_path_and_duration(\n",
        "    input_csv_path='/content/accent-recog-slt2022/CommonAccent/data_small/train_orig.csv',\n",
        "    output_csv_path='/content/accent-recog-slt2022/CommonAccent/data_small/train.csv',\n",
        "    base_audio_path=base_audio_path\n",
        ")\n",
        "\n",
        "# Dev\n",
        "update_csv_with_full_path_and_duration(\n",
        "    input_csv_path='/content/accent-recog-slt2022/CommonAccent/data_small/dev_orig.csv',\n",
        "    output_csv_path='/content/accent-recog-slt2022/CommonAccent/data_small/dev.csv',\n",
        "    base_audio_path=base_audio_path\n",
        ")\n",
        "\n",
        "# Test\n",
        "update_csv_with_full_path_and_duration(\n",
        "    input_csv_path='/content/accent-recog-slt2022/CommonAccent/data_small/test_orig.csv',\n",
        "    output_csv_path='/content/accent-recog-slt2022/CommonAccent/data_small/test.csv',\n",
        "    base_audio_path=base_audio_path\n",
        ")"
      ],
      "metadata": {
        "id": "NzNOJYwKNGWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/accent-recog-slt2022/CommonAccent\n",
        "# make .sh file executable:\n",
        "!chmod u+x run_accent_id_ecapa_tdnn.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS96svSH3Sti",
        "outputId": "5574daf0-8058-45b5-d46e-f3ef61be2d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/accent-recog-slt2022/CommonAccent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finally, we can run the model.\n",
        "You can try to run the training using cpu by adding\n",
        "\n",
        "--device=\"cpu\"\n",
        "\n",
        "to your .sh file (line 59)"
      ],
      "metadata": {
        "id": "3-LFB-nKEaHx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5Vz3bS0SBA4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e29693cc-d8d1-4bbe-a307-ddbd6eb27c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** About to start the training ***\n",
            "*** output folder: results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986 ***\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.pretrained.fetching - Fetch embedding_model.ckpt: Delegating to Huggingface hub, source speechbrain/spkrec-ecapa-voxceleb.\n",
            "speechbrain.utils.parameter_transfer - Loading pretrained files for: embedding_model\n",
            "speechbrain.core - Info: device arg overridden by command line input to: cuda:0\n",
            "speechbrain.core - 20.8M trainable parameters in AID\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "100% 29/29 [00:32<00:00,  1.13s/it, train_loss=7.51]\n",
            "100% 5/5 [00:02<00:00,  1.77it/s]\n",
            "speechbrain.utils.train_logger - Epoch: 1, lr: 1.00e-04 - train loss: 7.51 - valid loss: 4.81, valid error_rate: 2.67e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-41-42+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "100% 29/29 [00:31<00:00,  1.10s/it, train_loss=6.89]\n",
            "100% 5/5 [00:02<00:00,  2.48it/s]\n",
            "speechbrain.utils.train_logger - Epoch: 2, lr: 1.00e-04 - train loss: 6.89 - valid loss: 3.94, valid error_rate: 2.47e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-42-16+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-41-42+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100% 29/29 [00:33<00:00,  1.15s/it, train_loss=6.41]\n",
            "100% 5/5 [00:02<00:00,  2.38it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0001 to 9e-05\n",
            "speechbrain.utils.train_logger - Epoch: 3, lr: 1.00e-04 - train loss: 6.41 - valid loss: 3.44, valid error_rate: 3.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-42-52+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "100% 29/29 [00:34<00:00,  1.20s/it, train_loss=6.14]\n",
            "100% 5/5 [00:02<00:00,  1.95it/s]\n",
            "speechbrain.utils.train_logger - Epoch: 4, lr: 9.00e-05 - train loss: 6.14 - valid loss: 2.88, valid error_rate: 2.20e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-43-30+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-42-16+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-42-52+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "100% 29/29 [00:34<00:00,  1.18s/it, train_loss=5.85]\n",
            "100% 5/5 [00:02<00:00,  2.36it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-05 to 8.1e-05\n",
            "speechbrain.utils.train_logger - Epoch: 5, lr: 9.00e-05 - train loss: 5.85 - valid loss: 3.23, valid error_rate: 3.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-44-07+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-43-30+00\n",
            "100% 5/5 [00:02<00:00,  1.97it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 4 - test loss: 2.82, test error_rate: 1.60e-01\n",
            "Done training of speechbrain/spkrec-ecapa-voxceleb/embedding_model.ckpt in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986\n"
          ]
        }
      ],
      "source": [
        "!./run_accent_id_ecapa_tdnn.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the correct pretrained_path: /content/accent-recog-slt2022/CommonAccent/results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/\n",
        "# and load_folder: !ref <pretrained_path>/CKPT+2025-05-15+09-43-30+00\n",
        "# in the yaml file!\n",
        "# if run is successful, it will create an analysis folder (in your results folder)\n",
        "!python accent_id/inference.py accent_id/hparams/inference_ecapa_tdnn.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoByyoA_Rsls",
        "outputId": "15d01ebb-0300-4419-c3ad-9bc9902edcad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/analysis\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.pretrained.fetching - Fetch embedding_model.ckpt: Using existing file/symlink in model_checkpoints/embedding_model.ckpt.\n",
            "speechbrain.pretrained.fetching - Fetch classifier.ckpt: Using existing file/symlink in model_checkpoints/classifier.ckpt.\n",
            "speechbrain.pretrained.fetching - Fetch accent_encoder.txt: Using existing file/symlink in model_checkpoints/label_encoder.ckpt.\n",
            "speechbrain.utils.parameter_transfer - Loading pretrained files for: embedding_model, classifier, label_encoder\n",
            "speechbrain.core - Info: device arg from hparam file is used\n",
            "speechbrain.core - 20.8M trainable parameters in AccID_inf\n",
            "100% 5/5 [00:02<00:00,  1.88it/s]\n",
            "100% 5/5 [00:02<00:00,  2.47it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our project starts here!\n",
        "\n",
        "for 3 languages:\n",
        "*   **Russian**\n",
        "*   **Belarusian**\n",
        "*   **Serbian**"
      ],
      "metadata": {
        "id": "-p1-S2E2D-ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Serbian"
      ],
      "metadata": {
        "id": "zHKM7Vx8IVXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Russian"
      ],
      "metadata": {
        "id": "JrXWxfxoFDU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "tar_path = \"/content/cv-corpus-21.0-delta-2025-03-14-ru.tar\" #your downloaded locally dataset\n",
        "extract_path = \"/content/cv_data\"\n",
        "\n",
        "with tarfile.open(tar_path, \"r\") as tar:\n",
        "    tar.extractall(path=extract_path)\n",
        "\n",
        "print(\"Extraction complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbxykxcPRnuH",
        "outputId": "d5f133b3-dae1-4f0c-8830-cce825b921ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q speechbrain==0.5.13 \\\n",
        "    transformers==4.25.1 \\\n",
        "    librosa==0.9.2 \\\n",
        "    ipdb"
      ],
      "metadata": {
        "id": "1GaYRSeBS8Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "data_path = \"/content/cv_data/cv-corpus-21.0-delta-2025-03-14/ru\"\n",
        "\n",
        "validated = pd.read_csv(os.path.join(data_path, \"validated.tsv\"), sep='\\t')\n",
        "other = pd.read_csv(os.path.join(data_path, \"other.tsv\"), sep='\\t')\n",
        "\n",
        "print(f\"Validated samples: {len(validated)}\")\n",
        "print(f\"Other samples: {len(other)}\")\n",
        "\n",
        "df_full = pd.concat([validated, other], ignore_index=True)\n",
        "\n",
        "print(f\"Total samples combined: {len(df_full)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd_hd-3VYKiC",
        "outputId": "4320f1ca-6b3e-4b9d-bb14-68ce3bc37625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validated samples: 90\n",
            "Other samples: 2940\n",
            "Total samples combined: 3030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "data_path = \"/content/cv_data/cv-corpus-21.0-delta-2025-03-14/ru\"\n",
        "validated_path = os.path.join(data_path, \"validated.tsv\")\n",
        "other_path = os.path.join(data_path, \"other.tsv\")\n",
        "\n",
        "df_validated = pd.read_csv(validated_path, sep='\\t')\n",
        "df_other = pd.read_csv(other_path, sep='\\t')\n",
        "\n",
        "df_validated['source'] = 'validated'\n",
        "df_other['source'] = 'other'\n",
        "\n",
        "df_combined = pd.concat([df_validated, df_other], ignore_index=True)\n",
        "\n",
        "print(f\"Validated samples: {len(df_validated)}\")\n",
        "print(f\"Other samples: {len(df_other)}\")\n",
        "print(f\"Total samples combined: {len(df_combined)}\")\n",
        "\n",
        "# 70% train, 15% dev, 15% test\n",
        "df_train_ru, df_temp_ru = train_test_split(df_combined, test_size=0.3, random_state=42)\n",
        "df_dev_ru, df_test_ru = train_test_split(df_temp_ru, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Train size russian: {len(df_train_ru)}\")\n",
        "print(f\"Dev size russian: {len(df_dev_ru)}\")\n",
        "print(f\"Test size russian: {len(df_test_ru)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAB8p6cggAhr",
        "outputId": "17a0586a-8ebb-43d4-f978-2b321b8be423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validated samples: 90\n",
            "Other samples: 2940\n",
            "Total samples combined: 3030\n",
            "Train size russian: 2121\n",
            "Dev size russian: 454\n",
            "Test size russian: 455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Belarusian"
      ],
      "metadata": {
        "id": "bMcW-a2YFF_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loading the dataset locally"
      ],
      "metadata": {
        "id": "NzvYzJGgRLus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #use this if you want to upload the file from colab"
      ],
      "metadata": {
        "id": "oZ19BTktL5Gm",
        "outputId": "e7df1cfb-debc-4c9b-a527-94e8c389414c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gdrive_path = \"/content/drive/MyDrive/Colab Notebooks/cv-corpus-21.0-delta-2025-03-14-be.tar.gz\" #change to your location on Google Drive - look it up in the directory from the left\n",
        "gdrive_path = \"/content/drive/MyDrive/Colab Notebooks/cv-corpus-14.0-delta-2023-06-23-be.tar.gz\" #a smaller file\n",
        "extract_path = '/content/cv_data'\n",
        "import tarfile\n",
        "\n",
        "with tarfile.open(gdrive_path, \"r:gz\") as tar:\n",
        "    tar.extractall(path=extract_path)\n",
        "\n",
        "print(\"Extraction complete.\")"
      ],
      "metadata": {
        "id": "3RTQqKC7L9VW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbdf43a2-70f2-425a-b207-30c7481f3c7e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing from the directory\n",
        "#import tarfile\n",
        "\n",
        "#tar_path = \"/content/cv-corpus-21.0-delta-2025-03-14-be.tar.gz\" #your downloaded locally dataset\n",
        "#extract_path = \"/content/cv_data\n",
        "\n",
        "#with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "#    tar.extractall(path=extract_path)\n",
        "\n",
        "#print(\"Extraction complete.\")"
      ],
      "metadata": {
        "id": "ZThrQxQmJwQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q speechbrain==0.5.13 \\\n",
        "    transformers==4.25.1 \\\n",
        "    librosa==0.9.2 \\\n",
        "    ipdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3yeTiUBMg0p",
        "outputId": "0cefc021-3f63-4925-8dad-b59661db78ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.0/499.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "#data_path = \"/content/cv_data/cv-corpus-21.0-delta-2025-03-14/be\"\n",
        "data_path = \"/content/cv_data/cv-corpus-14.0-delta-2023-06-23/be\"\n",
        "\n",
        "validated = pd.read_csv(os.path.join(data_path, \"validated.tsv\"), sep='\\t')\n",
        "other = pd.read_csv(os.path.join(data_path, \"other.tsv\"), sep='\\t')\n",
        "\n",
        "print(f\"Validated samples: {len(validated)}\")\n",
        "print(f\"Other samples: {len(other)}\")\n",
        "\n",
        "df_full = pd.concat([validated, other], ignore_index=True)\n",
        "\n",
        "print(f\"Total samples combined: {len(df_full)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "priAK-urJGeY",
        "outputId": "5f48a912-51b1-41ff-f475-85a787aa442f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validated samples: 54028\n",
            "Other samples: 0\n",
            "Total samples combined: 54028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "#data_path = \"/content/cv_data/cv-corpus-21.0-delta-2025-03-14/be\"\n",
        "data_path = \"/content/cv_data/cv-corpus-14.0-delta-2023-06-23/be\"\n",
        "validated_path = os.path.join(data_path, \"validated.tsv\")\n",
        "other_path = os.path.join(data_path, \"other.tsv\")\n",
        "\n",
        "df_validated = pd.read_csv(validated_path, sep='\\t')\n",
        "df_other = pd.read_csv(other_path, sep='\\t')\n",
        "\n",
        "df_validated['source'] = 'validated'\n",
        "df_other['source'] = 'other'\n",
        "\n",
        "df_combined = pd.concat([df_validated, df_other], ignore_index=True)\n",
        "\n",
        "print(f\"Validated samples: {len(df_validated)}\")\n",
        "print(f\"Other samples: {len(df_other)}\")\n",
        "print(f\"Total samples combined: {len(df_combined)}\")\n",
        "\n",
        "# 70% train, 15% dev, 15% test\n",
        "df_train_be, df_temp_be = train_test_split(df_combined, test_size=0.3, random_state=42)\n",
        "df_dev_be, df_test_be = train_test_split(df_temp_be, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Train size belarusian: {len(df_train_be)}\")\n",
        "print(f\"Dev size belarusian: {len(df_dev_be)}\")\n",
        "print(f\"Test size belarusian: {len(df_test_be)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQVCdKm0OPgH",
        "outputId": "67796589-777e-4665-ffeb-97d5047ba949"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validated samples: 54028\n",
            "Other samples: 0\n",
            "Total samples combined: 54028\n",
            "Train size belarusian: 37819\n",
            "Dev size belarusian: 8104\n",
            "Test size belarusian: 8105\n",
            "train/dev/test CSVs created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_be.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "KFZ5YK0bT3to",
        "outputId": "ed267527-765d-4c5b-cc2a-2f2806643169"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               client_id  \\\n",
              "38167  4142df127af0c93b372ac7ef241884960419789dfb74de...   \n",
              "11293  2e0253b95a53a9f8d83ec5420566fdfb3763a22d5471b4...   \n",
              "51961  4142df127af0c93b372ac7ef241884960419789dfb74de...   \n",
              "34031  4142df127af0c93b372ac7ef241884960419789dfb74de...   \n",
              "24674  659b04f837f6b33da64c1cfd5472c09c5c611dc8dec68f...   \n",
              "\n",
              "                               path  \\\n",
              "38167  common_voice_be_37611380.mp3   \n",
              "11293  common_voice_be_37425952.mp3   \n",
              "51961  common_voice_be_37985849.mp3   \n",
              "34031  common_voice_be_37473095.mp3   \n",
              "24674  common_voice_be_37824009.mp3   \n",
              "\n",
              "                                                sentence up_votes down_votes  \\\n",
              "38167                     Мы з вамі маем іх чортаву моц.        3          0   \n",
              "11293                   Пра далейшы лёс інфармацыі няма.        2          0   \n",
              "51961  Узначальваў будаўніцтва і выкарыстанне першага...        2          0   \n",
              "34031  У дакуменце вызначалася, што ўсе мыты маюць іс...        2          0   \n",
              "24674                А на якой мове вучацца вашыя дзеці?        2          0   \n",
              "\n",
              "       age gender accents variant locale segment     source  \n",
              "38167  NaN    NaN     NaN     NaN     be     NaN  validated  \n",
              "11293  NaN    NaN     NaN     NaN     be     NaN  validated  \n",
              "51961  NaN    NaN     NaN     NaN     be     NaN  validated  \n",
              "34031  NaN    NaN     NaN     NaN     be     NaN  validated  \n",
              "24674  NaN    NaN     NaN     NaN     be     NaN  validated  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d9a0652-d8f8-46d3-a836-d42958fff81a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>client_id</th>\n",
              "      <th>path</th>\n",
              "      <th>sentence</th>\n",
              "      <th>up_votes</th>\n",
              "      <th>down_votes</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>accents</th>\n",
              "      <th>variant</th>\n",
              "      <th>locale</th>\n",
              "      <th>segment</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>38167</th>\n",
              "      <td>4142df127af0c93b372ac7ef241884960419789dfb74de...</td>\n",
              "      <td>common_voice_be_37611380.mp3</td>\n",
              "      <td>Мы з вамі маем іх чортаву моц.</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>be</td>\n",
              "      <td>NaN</td>\n",
              "      <td>validated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11293</th>\n",
              "      <td>2e0253b95a53a9f8d83ec5420566fdfb3763a22d5471b4...</td>\n",
              "      <td>common_voice_be_37425952.mp3</td>\n",
              "      <td>Пра далейшы лёс інфармацыі няма.</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>be</td>\n",
              "      <td>NaN</td>\n",
              "      <td>validated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51961</th>\n",
              "      <td>4142df127af0c93b372ac7ef241884960419789dfb74de...</td>\n",
              "      <td>common_voice_be_37985849.mp3</td>\n",
              "      <td>Узначальваў будаўніцтва і выкарыстанне першага...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>be</td>\n",
              "      <td>NaN</td>\n",
              "      <td>validated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34031</th>\n",
              "      <td>4142df127af0c93b372ac7ef241884960419789dfb74de...</td>\n",
              "      <td>common_voice_be_37473095.mp3</td>\n",
              "      <td>У дакуменце вызначалася, што ўсе мыты маюць іс...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>be</td>\n",
              "      <td>NaN</td>\n",
              "      <td>validated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24674</th>\n",
              "      <td>659b04f837f6b33da64c1cfd5472c09c5c611dc8dec68f...</td>\n",
              "      <td>common_voice_be_37824009.mp3</td>\n",
              "      <td>А на якой мове вучацца вашыя дзеці?</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>be</td>\n",
              "      <td>NaN</td>\n",
              "      <td>validated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d9a0652-d8f8-46d3-a836-d42958fff81a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0d9a0652-d8f8-46d3-a836-d42958fff81a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0d9a0652-d8f8-46d3-a836-d42958fff81a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c95c3832-66d6-45ca-bf90-c3875042c0a2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c95c3832-66d6-45ca-bf90-c3875042c0a2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c95c3832-66d6-45ca-bf90-c3875042c0a2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train_be",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## processing audio"
      ],
      "metadata": {
        "id": "U0vnPWT7fUo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In my file I had mp3 files instead of wav files, so I have to convert them to be processable by speechbrain (may be I have not just found the information about its ability to process mp3 files)"
      ],
      "metadata": {
        "id": "XvJHFZo7WrI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import shutil\n",
        "\n",
        "\n",
        "input_mp3_dir = '/content/cv_data/cv-corpus-14.0-delta-2023-06-23/be/clips'  # source .mp3 directory\n",
        "output_wav_dir = '/content/accent-recog-slt2022/CommonAccent/data_small/wav_files'  # target wav directory\n",
        "os.makedirs(output_wav_dir, exist_ok=True)\n",
        "\n",
        "# You can collect all file names from your filtered CSVs\n",
        "all_files = pd.concat([df_train_be, df_dev_be, df_test_be])[\"path\"].unique()\n",
        "\n",
        "# Loop through all .mp3 files and convert\n",
        "for file in os.listdir(input_mp3_dir):\n",
        "    if file.endswith('.mp3'):\n",
        "        mp3_path = os.path.join(input_mp3_dir, file)\n",
        "        wav_path = os.path.join(output_wav_dir, file.replace('.mp3', '.wav'))\n",
        "\n",
        "        try:\n",
        "            # Load MP3 using librosa (automatically resamples and converts to mono)\n",
        "            audio, sr = librosa.load(mp3_path, sr=16000, mono=True)\n",
        "            # Save as WAV 16kHz mono\n",
        "            sf.write(wav_path, audio, 16000)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to process {file}: {e}\")"
      ],
      "metadata": {
        "id": "zV1OGTYbg-0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from pydub import AudioSegment\n",
        "#import os\n",
        "#from tqdm import tqdm\n",
        "\n",
        "# mp3_dir = \"/content/cv_data/cv-corpus-14.0-delta-2023-06-23/be/clips\"\n",
        "# output_dir = \"/content/accent-recog-slt2022/CommonAccent/data/wav_files\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# # You can collect all file names from your filtered CSVs\n",
        "# all_files = pd.concat([df_train_be, df_dev_be, df_test_be])[\"path\"].unique()\n",
        "\n",
        "# for mp3_file in tqdm(all_files):\n",
        "#     mp3_path = os.path.join(mp3_dir, mp3_file)\n",
        "#     wav_path = os.path.join(output_dir, mp3_file.replace(\".mp3\", \".wav\"))\n",
        "\n",
        "#     if not os.path.exists(mp3_path):\n",
        "#         continue  # skip if the file isn't found\n",
        "\n",
        "#     try:\n",
        "#         audio = AudioSegment.from_mp3(mp3_path)\n",
        "#         audio.export(wav_path, format=\"wav\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Failed: {mp3_file} -> {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "bWj5ehbhWxxf",
        "outputId": "4efe90c7-5283-44d0-e5fb-ce332f67464c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 8437/54028 [47:15<4:15:24,  2.97it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-54-591632987.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_mp3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp3_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydub/audio_segment.py\u001b[0m in \u001b[0;36mfrom_mp3\u001b[0;34m(cls, file, parameters)\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_mp3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mp3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydub/audio_segment.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmediainfo_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_ahead_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_ahead_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             audio_streams = [x for x in info['streams']\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydub/utils.py\u001b[0m in \u001b[0;36mmediainfo_json\u001b[0;34m(filepath, read_ahead_limit)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprober\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-of'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'json'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcommand_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdin_parameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdin_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wav_folder = output_wav_dir  # path to saved .wav files\n",
        "for df in [df_train_be, df_dev_be, df_test_be]:\n",
        "    df[\"wav\"] = df[\"path\"].apply(lambda x: os.path.join(wav_folder, x.replace(\".mp3\", \".wav\")))"
      ],
      "metadata": {
        "id": "XF_BkFBGW8Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_be.head()"
      ],
      "metadata": {
        "id": "7FRiK4nhZXqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to the path your later code expects\n",
        "output_dir = \"/content/accent-recog-slt2022/CommonAccent/data\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "df_train_be.to_csv(os.path.join(output_dir, \"train.csv\"), index=False)\n",
        "df_dev_be.to_csv(os.path.join(output_dir, \"dev.csv\"), index=False)\n",
        "df_test_be.to_csv(os.path.join(output_dir, \"test.csv\"), index=False)"
      ],
      "metadata": {
        "id": "r6OgiuxoZUPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/data_small.zip /content/accent-recog-slt2022/CommonAccent/data_small\n",
        "from google.colab import files\n",
        "files.download(\"/content/data_small.zip\")"
      ],
      "metadata": {
        "id": "ODxhyxnoewnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -R /content/accent-recog-slt2022/CommonAccent/data_small/wav_files /content/drive/MyDrive/.../data/wav"
      ],
      "metadata": {
        "id": "H0fVjQ8Sgq8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accent extraction - failure\n",
        "\n",
        "common_accent_prepare.py is a script in the loaded repo\n",
        "it is situated here: /content/accent-recog-slt2022/CommonAccent/common_accent_prepare.py\n",
        "\n",
        "```\n",
        "def prepare_common_accent(\n",
        "        data_folder,\n",
        "        save_folder,\n",
        "        accented_letters=False,\n",
        "        language=\"en\",        \n",
        "        skip_prep=False,\n",
        "    ):\n",
        "\n",
        "    \"\"\"\n",
        "    Prepares the csv files for the CommonAccent dataset for Accent Classification.\n",
        "    Download: https://commonvoice.mozilla.org/en/datasets\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    data_folder : str\n",
        "        Path to the folder where the CommonAccent dataset for Accent Classification is stored.\n",
        "        This path should include the multi: /datasets/CommonAccent\n",
        "    save_folder : str\n",
        "        The directory where to store the csv files.\n",
        "    max_duration : int, optional\n",
        "        Max duration (in seconds) of training uterances.\n",
        "    accented_letters : bool, optional\n",
        "        Defines if accented letters will be kept as individual letters or\n",
        "        transformed to the closest non-accented letters.\n",
        "    language: str\n",
        "        Specify the language for text normalization.        \n",
        "    skip_prep: bool\n",
        "        If True, skip data preparation.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> from recipes.CommonAccent.common_accent_prepare import prepare_common_accent\n",
        "    >>> data_folder = '/datasets/CommonAccent'\n",
        "    >>> save_folder = 'exp/CommonAccent_exp'\n",
        "    >>> prepare_common_accent(\\\n",
        "            data_folder,\\\n",
        "            save_folder,\\\n",
        "            skip_prep=False\\\n",
        "        )\n",
        "    \"\"\"\n",
        "```\n"
      ],
      "metadata": {
        "id": "4IY89c7CIqIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/accent-recog-slt2022\")\n",
        "\n",
        "from CommonAccent.common_accent_prepare import prepare_common_accent\n",
        "\n",
        "prepare_common_accent(\n",
        "    data_folder=\"/content/cv_data/\",\n",
        "    save_folder=\"/content/common_accent_output\",\n",
        "    language=\"be\",\n",
        "    skip_prep=False\n",
        ")"
      ],
      "metadata": {
        "id": "F79x36GGn88O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "a70a919a-d485-4913-a3cc-aca1135212bc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "/content/cv_data//be must have at least 'train.tsv' file in it.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-27-3604352227.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mCommonAccent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon_accent_prepare\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprepare_common_accent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m prepare_common_accent(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdata_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/cv_data/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msave_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/common_accent_output\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/accent-recog-slt2022/CommonAccent/common_accent_prepare.py\u001b[0m in \u001b[0;36mprepare_common_accent\u001b[0;34m(data_folder, save_folder, accented_letters, language, skip_prep)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;31m# Additional checks to make sure the data folder contains Common Accent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mcheck_common_accent_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Audio files extensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/accent-recog-slt2022/CommonAccent/common_accent_prepare.py\u001b[0m in \u001b[0;36mcheck_common_accent_folder\u001b[0;34m(data_folder, language)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"train.tsv\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{data_folder}/{language} must have at least 'train.tsv' file in it.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: /content/cv_data//be must have at least 'train.tsv' file in it."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python common_accent_prepare.py --language \"be\" /content/cv_data /content/common_accent_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxbTh_Vnh9_b",
        "outputId": "7668f949-504d-40c8-c185-4f8fd03f1c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/accent-recog-slt2022/CommonAccent/common_accent_prepare.py\", line 570, in <module>\n",
            "    main()\n",
            "  File \"/content/accent-recog-slt2022/CommonAccent/common_accent_prepare.py\", line 464, in main\n",
            "    prepare_common_accent(args.cv_folder, args.output_folder, language=args.language)\n",
            "  File \"/content/accent-recog-slt2022/CommonAccent/common_accent_prepare.py\", line 169, in prepare_common_accent\n",
            "    check_common_accent_folder(data_folder, language=language)\n",
            "  File \"/content/accent-recog-slt2022/CommonAccent/common_accent_prepare.py\", line 440, in check_common_accent_folder\n",
            "    raise FileNotFoundError(err_msg)\n",
            "FileNotFoundError: /content/cv_data/be must have at least 'train.tsv' file in it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunatlelly, the scripts doen ot have an option for our chosen langauge, so we decided to focus on just langauge recognition of Russian, Belarusian and Serbian"
      ],
      "metadata": {
        "id": "8Ec5DfTcH7DD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: loading from HuggingFace"
      ],
      "metadata": {
        "id": "DXTmepMXROnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "download_data_hf.py is a script in the loaded repo\n",
        "it is situated here: /content/accent-recog-slt2022/CommonAccent/download_data_hf.py\n",
        "\n",
        "**use the code from HugginFace and the number of the dataset you need, e.g.19  as in the script**\n",
        "https://huggingface.co/datasets/fsicoli/common_voice_19_0\n",
        "\n",
        "\n",
        "```\n",
        "\"\"\"\n",
        "Script to download the CommonVoice dataset using Hugging Face\n",
        "\n",
        "At Hugging Face: https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0\n",
        "Download the dataset: https://commonvoice.mozilla.org/en/datasets\n",
        "\n",
        "def prepare_cv_from_hf(output_folder, language=\"en\"):\n",
        "    \"\"\" function to prepare the datasets in <output-folder> \"\"\"\n",
        "\n",
        "    output_folder = os.path.join(output_folder, language)\n",
        "    # create the output folder: in case is not present\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Prepare the the common voice dataset in streaming mode\n",
        "    # common_voice_ds = load_dataset(_COMMON_VOICE_FOLDER, language, streaming=True)\n",
        "    common_voice_ds = load_dataset(\"fsicoli/common_voice_19_0\", language, streaming=True, trust_remote_code=True)\n",
        "\n",
        "    !!!!\n",
        "    it uses here already a dataset\n",
        "\n",
        "    # just select relevant splits: train/validation/test set\n",
        "    splits = [\"train\", \"validation\", \"test\"]\n",
        "    common_voice = {}\n",
        "    \n",
        "    # load, prepare and filter each split in streaming mode:\n",
        "    for split in splits:\n",
        "        # filter out samples without accent\n",
        "        ds = common_voice_ds[split].filter( lambda x: x['accent'] != '')\n",
        "        common_voice[split] = ds\n",
        "        \n",
        "    for dataset in common_voice:\n",
        "        csv_lines = []\n",
        "        # Starting index\n",
        "        idx = 0\n",
        "        for sample in common_voice[dataset]:\n",
        "            # get path and utt_id\n",
        "            mp3_path = sample['path']\n",
        "            utt_id = mp3_path.split(\".\")[-2].split(\"/\")[-1]            \n",
        "            \n",
        "            # Create a row with metadata + transcripts\n",
        "            csv_line = [\n",
        "                idx,  # ID\n",
        "                utt_id,  # Utterance ID\n",
        "                mp3_path,  # File name\n",
        "                sample[\"locale\"],\n",
        "                sample[\"accent\"],\n",
        "                sample[\"age\"],\n",
        "                sample[\"gender\"],\n",
        "                sample[\"sentence\"], # transcript\n",
        "            ]\n",
        "\n",
        "            # Adding this line to the csv_lines list\n",
        "            csv_lines.append(csv_line)\n",
        "            # Increment index\n",
        "            idx += 1\n",
        "\n",
        "        # CSV column titles\n",
        "        csv_header = [\"idx\", \"utt_id\", \"mp3_path\", \"language\", \"accent\", \"age\", \"gender\", \"transcript\"]\n",
        "        # Add titles to the list at indexx 0\n",
        "        csv_lines.insert(0, csv_header)\n",
        "\n",
        "        # Writing the csv lines\n",
        "        csv_file = os.path.join(output_folder, dataset+'.tsv')\n",
        "\n",
        "        with open(csv_file, mode=\"w\", encoding=\"utf-8\") as csv_f:\n",
        "            csv_writer = csv.writer(\n",
        "                csv_f, delimiter=\"\\t\", quotechar='\"', quoting=csv.QUOTE_MINIMAL\n",
        "            )\n",
        "            for line in csv_lines:\n",
        "                csv_writer.writerow(line)\n",
        "    print(f\"Prepare CommonVoice: for {language} in {output_folder}\")\n",
        "\n",
        "def main():\n",
        "    # read input from CLI, you need to run it from the command lind\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # reporting vars\n",
        "    parser.add_argument(\n",
        "        \"--language\",\n",
        "        type=str,\n",
        "        default='en',\n",
        "        help=\"Language to load\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"output_folder\",\n",
        "        help=\"path of the output folder to store the csv files for each split\",\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "tjn_F7-cRozj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So, the script loads already one dataset\n",
        "    common_voice_ds = load_dataset(\"fsicoli/common_voice_19_0\", language, streaming=True, trust_remote_code=True), therefore you can run the cell below the same as it was in the original notebook.\n",
        "    If you need, however, abother dataset, e.g. wit hmore audio files, you need copy the code above and specify it and run the code without a script but your code in the cells**"
      ],
      "metadata": {
        "id": "DOnFOrZ99Slu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from datasets import load_dataset\n",
        "#common_voice_ds = load_dataset(\"mozilla-foundation/common_voice_19_0\", \"be\")"
      ],
      "metadata": {
        "id": "0snm1YFum2sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_cv_from_hf(output_folder, language=\"en\"):\n",
        "\n",
        "    output_folder = os.path.join(output_folder, language)\n",
        "    # create the output folder: in case is not present\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Prepare the the common voice dataset in streaming mode\n",
        "    common_voice_ds = load_dataset(_COMMON_VOICE_FOLDER, language, streaming=True)\n",
        "    #common_voice_ds = load_dataset(\"fsicoli/common_voice_19_0\", language, streaming=True, trust_remote_code=True)\n",
        "\n",
        "    # just select relevant splits: train/validation/test set\n",
        "    splits = [\"train\", \"validation\", \"test\"]\n",
        "    common_voice = {}\n",
        "\n",
        "    # load, prepare and filter each split in streaming mode:\n",
        "    for split in splits:\n",
        "        # filter out samples without accent\n",
        "        ds = common_voice_ds[split].filter( lambda x: x['accent'] != '')\n",
        "        common_voice[split] = ds\n",
        "\n",
        "    for dataset in common_voice:\n",
        "        csv_lines = []\n",
        "        # Starting index\n",
        "        idx = 0\n",
        "        for sample in common_voice[dataset]:\n",
        "            # get path and utt_id\n",
        "            mp3_path = sample['path']\n",
        "            utt_id = mp3_path.split(\".\")[-2].split(\"/\")[-1]\n",
        "\n",
        "            # Create a row with metadata + transcripts\n",
        "            csv_line = [\n",
        "                idx,  # ID\n",
        "                utt_id,  # Utterance ID\n",
        "                mp3_path,  # File name\n",
        "                sample[\"locale\"],\n",
        "                sample[\"accent\"],\n",
        "                sample[\"age\"],\n",
        "                sample[\"gender\"],\n",
        "                sample[\"sentence\"], # transcript\n",
        "            ]\n",
        "\n",
        "            # Adding this line to the csv_lines list\n",
        "            csv_lines.append(csv_line)\n",
        "            # Increment index\n",
        "            idx += 1\n",
        "\n",
        "        # CSV column titles\n",
        "        csv_header = [\"idx\", \"utt_id\", \"mp3_path\", \"language\", \"accent\", \"age\", \"gender\", \"transcript\"]\n",
        "        # Add titles to the list at indexx 0\n",
        "        csv_lines.insert(0, csv_header)\n",
        "\n",
        "        # Writing the csv lines\n",
        "        csv_file = os.path.join(output_folder, dataset+'.tsv')\n",
        "\n",
        "        with open(csv_file, mode=\"w\", encoding=\"utf-8\") as csv_f:\n",
        "            csv_writer = csv.writer(\n",
        "                csv_f, delimiter=\"\\t\", quotechar='\"', quoting=csv.QUOTE_MINIMAL\n",
        "            )\n",
        "            for line in csv_lines:\n",
        "                csv_writer.writerow(line)\n",
        "    print(f\"Prepare CommonVoice: for {language} in {output_folder}\")"
      ],
      "metadata": {
        "id": "ixQ5d9XfPx1q"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare_cv_from_hf(\"/content/cv_data\", language=\"be\", data_dir=\"/content/cv_data/cv-corpus-14.0-delta-2023-06-23/be\") still loads the HugginFace"
      ],
      "metadata": {
        "id": "f-FkYsWMMJd-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python /content/accent-recog-slt2022/CommonAccent/download_data_hf.py --language \"be\" /content/cv_data #loads friscoli/common_voice 19\n",
        "\n",
        "#it is an original code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FTuIFlzm_Ue",
        "outputId": "7940c43e-538c-4cd5-dea7-7ac97be875a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/accent-recog-slt2022/CommonAccent/download_data_hf.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torchaudio\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import write\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Paths\n",
        "data_path = \"/content/accent-recog-slt2022/CommonAccent/data/\"\n",
        "audio_root = \"/content/cv_data/cv-corpus-21.0-delta-2025-03-14/be/clips/\"\n",
        "save_dir = os.path.join(data_path, \"wav_files\")\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Load CSVs\n",
        "df_train = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
        "df_test = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n",
        "df_dev = pd.read_csv(os.path.join(data_path, \"dev.csv\"))\n",
        "\n",
        "# Combine all file references\n",
        "all_files = pd.concat([df_train, df_test, df_dev])\n",
        "unique_filenames = all_files[\"wav\"].apply(os.path.basename).unique()\n",
        "\n",
        "# Convert and save as .wav\n",
        "for file_name in tqdm(unique_filenames, desc=\"Converting audio\"):\n",
        "    mp3_path = os.path.join(audio_root, file_name)\n",
        "    if not os.path.isfile(mp3_path):\n",
        "        print(f\"⚠️ Missing file: {mp3_path}\")\n",
        "        continue\n",
        "    try:\n",
        "        waveform, sample_rate = torchaudio.load(mp3_path)\n",
        "        wav_path = os.path.join(save_dir, file_name.replace(\".mp3\", \".wav\"))\n",
        "        torchaudio.save(wav_path, waveform, sample_rate)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {mp3_path}: {e}\")"
      ],
      "metadata": {
        "id": "AJRhPfd2J6yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import write\n",
        "from datasets import load_dataset\n",
        "\n",
        "data_path = \"/content/accent-recog-slt2022/CommonAccent/data/\"\n",
        "\n",
        "# Load CSV\n",
        "csv_path_train = data_path + \"train.csv\"\n",
        "df_train = pd.read_csv(csv_path_train)\n",
        "file_list_train = list(set(df_train['wav']))\n",
        "\n",
        "csv_path_test = data_path + \"test.csv\"\n",
        "df_test = pd.read_csv(csv_path_test)\n",
        "file_list_test = list(set(df_test['wav']))\n",
        "\n",
        "csv_path_dev = data_path + \"dev.csv\"\n",
        "df_dev = pd.read_csv(csv_path_dev)\n",
        "file_list_dev = list(set(df_dev['wav']))\n",
        "\n",
        "# Set save directory\n",
        "save_dir = data_path + \"wav_files\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Load dataset in streaming mode\n",
        "language = \"be\"  # or whatever language code you're using\n",
        "cv_11_be_train = load_dataset(\"mozilla-foundation/common_voice_11_0\", language, streaming=True, split=\"train\", trust_remote_code=True)\n",
        "print(next(iter(cv_11_be_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ad6a31e-3f22-402f-fd92-e8410005b469",
        "id": "J9dhLrqeFNvM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Reading metadata...: 171388it [00:07, 23662.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'client_id': 'c14f21cacf2b7701ad0bead0dd1b31ec9d3a1557708e446de55e98b4b470cf31072c82543e5ba518c5c187a91868878a4e32727054a3dd94f9df41c9a13d8c62', 'path': 'it_train_0/common_voice_it_17415777.mp3', 'audio': {'path': 'it_train_0/common_voice_it_17415777.mp3', 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
            "       -1.61342832e-05, -1.57291943e-05, -1.16234423e-05]), 'sampling_rate': 48000}, 'sentence': \"Il marchese aveva già moglie in quell'epoca?\", 'up_votes': 3, 'down_votes': 0, 'age': '', 'gender': '', 'accent': '', 'locale': 'it', 'segment': '', 'variant': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_11_be_dev = load_dataset(\"mozilla-foundation/common_voice_11_0\", language, streaming=False, split=\"validation\", trust_remote_code=True)\n",
        "cv_11_be_test = load_dataset(\"mozilla-foundation/common_voice_11_0\", language, streaming=False, split=\"test\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "4Z2evoZ9FNvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Count entries per accent\n",
        "accent_counts = df_train['accent'].value_counts()\n",
        "\n",
        "# Print the result\n",
        "print(accent_counts)\n",
        "\n",
        "# select randomly max 300 samples for each accent that has more than 300 samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "003c3702-9ce5-40c1-8950-9693771089f4",
        "id": "myZLU5jiFNvM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accent\n",
            "TENDENTE AL SICULO MA NON MARCATO    1801\n",
            "BASILICATA TRENTINO                  1801\n",
            "VENETO                               1393\n",
            "MERIDIONALE                           134\n",
            "EMILIANO                               91\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## balancing\n",
        "\n",
        "Apparently, we don't need this part as we don't have accents and a suitable scripts for this"
      ],
      "metadata": {
        "id": "xRW333kiUKTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# takes 2 min for it dev, test\n",
        "for item1 in cv_19_dev:\n",
        "    filename = (item1['path'].split('/')[-2] + '/' + item1['path'].split('/')[-1])\n",
        "    if filename in file_list_train or filename in file_list_test or filename in file_list_dev:\n",
        "      # Decode audio\n",
        "      audio_array = item1['audio']['array']  # Audio waveform\n",
        "      sampling_rate = item1['audio']['sampling_rate']  # Sampling rate\n",
        "\n",
        "      # Create the filename\n",
        "      file_name = os.path.join(save_dir, filename)\n",
        "      os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "\n",
        "      # Save the audio as a WAV file\n",
        "      write(file_name, sampling_rate, audio_array.astype(np.float32))  # Save as float32 for WAV compatibility"
      ],
      "metadata": {
        "id": "BcDxGKE4fJ0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item1 in cv_19_test:\n",
        "    filename = (item1['path'].split('/')[-2] + '/' + item1['path'].split('/')[-1])\n",
        "    if filename in file_list_train or filename in file_list_test or filename in file_list_dev:\n",
        "      # Decode audio\n",
        "      audio_array = item1['audio']['array']  # Audio waveform\n",
        "      sampling_rate = item1['audio']['sampling_rate']  # Sampling rate\n",
        "\n",
        "      # Create the filename\n",
        "      file_name = os.path.join(save_dir, filename)\n",
        "      os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "\n",
        "      # Save the audio as a WAV file\n",
        "      write(file_name, sampling_rate, audio_array.astype(np.float32))  # Save as float32 for WAV compatibility"
      ],
      "metadata": {
        "id": "az85pzwpfJ0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training"
      ],
      "metadata": {
        "id": "XyNqehXCT2wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pandas==2.2.2 \\\n",
        "    huggingface_hub>=0.7.0 \\\n",
        "    hyperpyyaml==0.0.1 \\\n",
        "    joblib \\\n",
        "    numpy==2.0.2 \\\n",
        "    packaging \\\n",
        "    pre-commit==2.3.0 \\\n",
        "    sentencepiece>=0.1.91 \\\n",
        "    SoundFile \\\n",
        "    tqdm"
      ],
      "metadata": {
        "id": "0OGFpYuJTLLt"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "gTnWihC0TLLu"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2363280-227e-4528-f9b2-90061f883fc8",
        "id": "-ieiVxJUT7iF"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ./run_accent_id_ecapa_tdnn.sh: Permission denied\n"
          ]
        }
      ],
      "source": [
        "!./run_accent_id_ecapa_tdnn.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the correct pretrained_path: /content/accent-recog-slt2022/CommonAccent/results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/\n",
        "# and load_folder: !ref <pretrained_path>/CKPT+2025-05-15+09-43-30+00\n",
        "# in the yaml file!\n",
        "# if run is successful, it will create an analysis folder (in your results folder)\n",
        "!python accent_id/inference.py accent_id/hparams/inference_ecapa_tdnn.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d01ebb-0300-4419-c3ad-9bc9902edcad",
        "id": "Fr3KYxN2T7iF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/analysis\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.pretrained.fetching - Fetch embedding_model.ckpt: Using existing file/symlink in model_checkpoints/embedding_model.ckpt.\n",
            "speechbrain.pretrained.fetching - Fetch classifier.ckpt: Using existing file/symlink in model_checkpoints/classifier.ckpt.\n",
            "speechbrain.pretrained.fetching - Fetch accent_encoder.txt: Using existing file/symlink in model_checkpoints/label_encoder.ckpt.\n",
            "speechbrain.utils.parameter_transfer - Loading pretrained files for: embedding_model, classifier, label_encoder\n",
            "speechbrain.core - Info: device arg from hparam file is used\n",
            "speechbrain.core - 20.8M trainable parameters in AccID_inf\n",
            "100% 5/5 [00:02<00:00,  1.88it/s]\n",
            "100% 5/5 [00:02<00:00,  2.47it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluation with confusion matrix, error rate, precision, and F1-score"
      ],
      "metadata": {
        "id": "ED59kUemC3jW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## visualisation wit t-sne"
      ],
      "metadata": {
        "id": "SrJsf6iCUma6"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}