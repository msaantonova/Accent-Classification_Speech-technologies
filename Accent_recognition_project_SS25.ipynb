{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msaantonova/Accent-Classification_Speech-technologies/blob/main/Accent_recognition_project_SS25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "utPRj8QSfBW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repository from the paper\n",
        "\n",
        "\"This repository provides all the necessary tools to perform accent identification from speech recordings with SpeechBrain toolkit! The system uses a model fine-tuned on the CommonAccent dataset in English (21 accents). The provided system can recognize the following 21 accents of English from short speech recordings\""
      ],
      "metadata": {
        "id": "6uOohfJNCNLx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvKoxtf7-l7s",
        "outputId": "9ad8a4f9-f18b-4ca9-d270-f485e216ca81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'accent-recog-slt2022' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/lgspeech/accent-recog-slt2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAG2EUzBeABn",
        "outputId": "b7815b1a-c5d0-4edb-f97f-dea3b04c9009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/accent-recog-slt2022\n",
            "Ignoring SoundFile: markers 'sys_platform == \"win32\"' don't match your environment\n",
            "Collecting speechbrain==0.5.13 (from -r requirements.txt (line 1))\n",
            "  Using cached speechbrain-0.5.13-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting datasets==2.8.0 (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-2.8.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting transformers==4.25.1 (from -r requirements.txt (line 3))\n",
            "  Using cached transformers-4.25.1-py3-none-any.whl.metadata (93 kB)\n",
            "Collecting librosa==0.9.2 (from -r requirements.txt (line 4))\n",
            "  Using cached librosa-0.9.2-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting ipdb>=0.13.9 (from -r requirements.txt (line 5))\n",
            "  Using cached ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.2.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.33.0)\n",
            "Collecting hyperpyyaml>=0.0.1 (from -r requirements.txt (line 8))\n",
            "  Using cached HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (1.5.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (24.2)\n",
            "Collecting pre-commit>=2.3.0 (from -r requirements.txt (line 12))\n",
            "  Downloading pre_commit-4.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting scipy<1.9,>=1.4.1 (from -r requirements.txt (line 13))\n",
            "  Downloading scipy-1.6.1.tar.gz (27.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.3/27.3 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (2.6.0+cu124)\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 2.0.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchaudio<=0.11.0,>=0.9.0 (from versions: 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchaudio<=0.11.0,>=0.9.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%cd /content/accent-recog-slt2022\n",
        "!python -m pip install -r requirements.txt\n",
        "# you can ignore any errors for now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQrLs6ilfGon",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d0a248-8cce-4d43-9885-ac725ab5fc57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/474.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m471.0/474.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/177.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.6.1 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets==3.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change working directory:\n",
        "%cd CommonAccent/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m6-V5CHYS-V",
        "outputId": "5005dff4-74a9-47e4-c540-fec563516faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/accent-recog-slt2022/CommonAccent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the data (or stream) and create the list of filtered items\n",
        "\n",
        "You can either use streaming, or save the files locally. you can check at common voice, how big the corpus for each language is.\n",
        "If you use streaming, it  will take a few hours.\n",
        "\n",
        "The aim of it is to create lists of all audios that have marked accent in the metadata, so you can choose which ones of these you want for classification."
      ],
      "metadata": {
        "id": "gAQzxiFMYUnj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAt-X0eXezdf",
        "outputId": "692d63c8-5371-43ff-841e-c3dee49fc101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/download_data_hf.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "#this part descrbes the dataset index and the language\n",
        "!python download_data_hf.py --language \"it\" data/cv_11/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the filtered list to be used in this model --> create csv files in correct form"
      ],
      "metadata": {
        "id": "xDqntrpPYjUn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Zgu6_R0J6dP_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "265510a5-72a1-455e-cfbd-946825821583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.0/499.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/363.4 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute '_str'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3796896608>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -q speechbrain==0.5.13      transformers==4.25.1      librosa==0.9.2      ipdb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     return {\n\u001b[1;32m   1086\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     }\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    605\u001b[0m             make_files(\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \"\"\"\n\u001b[1;32m   1234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \"\"\"\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m__fspath__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mas_posix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             self._str = self._format_parsed_parts(self._drv, self._root,\n\u001b[0m\u001b[1;32m    543\u001b[0m                                                   self._parts) or '.'\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m_format_parsed_parts\u001b[0;34m(cls, drv, root, parts)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_format_parsed_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdrv\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!pip install -q speechbrain==0.5.13 \\\n",
        "    transformers==4.25.1 \\\n",
        "    librosa==0.9.2 \\\n",
        "    ipdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python common_accent_prepare.py --language \"it\" data/cv_11 data/"
      ],
      "metadata": {
        "id": "TJ25UXrSYQpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import write\n",
        "from datasets import load_dataset\n",
        "\n",
        "data_path = \"/content/accent-recog-slt2022/CommonAccent/data/\"\n",
        "# Load CSV\n",
        "csv_path_train = data_path + \"train.csv\"\n",
        "df_train = pd.read_csv(csv_path_train)\n",
        "file_list_train = list(set(df_train['wav']))\n",
        "\n",
        "csv_path_test = data_path + \"test.csv\"\n",
        "df_test = pd.read_csv(csv_path_test)\n",
        "file_list_test = list(set(df_test['wav']))\n",
        "\n",
        "csv_path_dev = data_path + \"dev.csv\"\n",
        "df_dev = pd.read_csv(csv_path_dev)\n",
        "file_list_dev = list(set(df_dev['wav']))\n",
        "\n",
        "# Set save directory\n",
        "save_dir = data_path + \"wav_files\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Load dataset in streaming mode\n",
        "language = \"it\"  # or whatever language code you're using\n",
        "cv_19_train = load_dataset(\"fsicoli/common_voice_19_0\", language, streaming=True, split=\"train\", trust_remote_code=True)\n",
        "print(next(iter(cv_19_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pHghJSvEuEq",
        "outputId": "6ad6a31e-3f22-402f-fd92-e8410005b469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Reading metadata...: 171388it [00:07, 23662.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'client_id': 'c14f21cacf2b7701ad0bead0dd1b31ec9d3a1557708e446de55e98b4b470cf31072c82543e5ba518c5c187a91868878a4e32727054a3dd94f9df41c9a13d8c62', 'path': 'it_train_0/common_voice_it_17415777.mp3', 'audio': {'path': 'it_train_0/common_voice_it_17415777.mp3', 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
            "       -1.61342832e-05, -1.57291943e-05, -1.16234423e-05]), 'sampling_rate': 48000}, 'sentence': \"Il marchese aveva già moglie in quell'epoca?\", 'up_votes': 3, 'down_votes': 0, 'age': '', 'gender': '', 'accent': '', 'locale': 'it', 'segment': '', 'variant': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_19_dev = load_dataset(\"fsicoli/common_voice_19_0\", language, streaming=False, split=\"validation\", trust_remote_code=True)\n",
        "cv_19_test = load_dataset(\"fsicoli/common_voice_19_0\", language, streaming=False, split=\"test\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "PMZFhNN_V2Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Count entries per accent\n",
        "accent_counts = df_train['accent'].value_counts()\n",
        "\n",
        "# Print the result\n",
        "print(accent_counts)\n",
        "\n",
        "# select randomly max 300 samples for each accent that has more than 300 samples\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Uyu3fx-SAlo",
        "outputId": "003c3702-9ce5-40c1-8950-9693771089f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accent\n",
            "TENDENTE AL SICULO MA NON MARCATO    1801\n",
            "BASILICATA TRENTINO                  1801\n",
            "VENETO                               1393\n",
            "MERIDIONALE                           134\n",
            "EMILIANO                               91\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This code adjusts the audio paths and can be used for balancing the data:"
      ],
      "metadata": {
        "id": "1bk_7sQqCp-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# add /content/accent-recog-slt2022/CommonAccent/data_small to each \"wav\" in the csv .../train.csv\n",
        "save_dir_colab = data_path\n",
        "# \"content/accent-recog-slt2022/CommonAccent/data_small/\"\n",
        "df_train['wav'] = df_train['wav'].apply(lambda x: os.path.join(save_dir_colab, x))\n",
        "df_test['wav'] = df_test['wav'].apply(lambda x: os.path.join(save_dir_colab, x))\n",
        "df_dev['wav'] = df_dev['wav'].apply(lambda x: os.path.join(save_dir_colab, x))\n",
        "df_train.to_csv(csv_path_train, index=False)\n",
        "\n",
        "# show all available accents\n",
        "accent_counts = df_train['accent'].value_counts()\n",
        "\n",
        "# Print the result\n",
        "print(accent_counts)\n",
        "\n",
        "# select randomly max 300 samples for each accent\n",
        "df_train2 = df_train.groupby('accent').apply(lambda x: x.sample(min(len(x), 300), random_state=1)).reset_index(drop=True)\n",
        "df_dev2 = df_dev.groupby('accent').apply(lambda x: x.sample(min(len(x), 50), random_state=1)).reset_index(drop=True)\n",
        "df_test2 = df_test.groupby('accent').apply(lambda x: x.sample(min(len(x), 50), random_state=1)).reset_index(drop=True)\n",
        "# remove accents with less than 300 samples\n",
        "df_train2 = df_train2.groupby('accent').filter(lambda x: len(x) >= 300)\n",
        "# get names of remaining accents\n",
        "remaining_accents = df_train2['accent'].unique()\n",
        "# filter dev and test sets to keep only the remaining accents\n",
        "df_dev2 = df_dev2[df_dev2['accent'].isin(remaining_accents)]\n",
        "df_test2 = df_test2[df_test2['accent'].isin(remaining_accents)]\n",
        "# reduce to 50 samples for dev and test sets\n",
        "\n",
        "# print stats of the new dataframe\n",
        "accent_counts2_train = df_train2['accent'].value_counts()\n",
        "accent_counts2_dev = df_dev2['accent'].value_counts()\n",
        "accent_counts2_test = df_test2['accent'].value_counts()\n",
        "# Print the result\n",
        "print(accent_counts2_train)\n",
        "print(accent_counts2_dev)\n",
        "print(accent_counts2_test)\n",
        "\n",
        "# save csv again with these samples\n",
        "os.makedirs('/content/accent-recog-slt2022/CommonAccent/data_small', exist_ok=True)\n",
        "df_train2.to_csv('/content/accent-recog-slt2022/CommonAccent/data_small/train.csv', index=False)\n",
        "df_dev2.to_csv('/content/accent-recog-slt2022/CommonAccent/data_small/dev.csv', index=False)\n",
        "df_test2.to_csv('/content/accent-recog-slt2022/CommonAccent/data_small/test.csv', index=False)\n",
        "# download files to disk\n",
        "!zip -r /content/accent-recog-slt2022/CommonAccent/data_small.zip /content/accent-recog-slt2022/CommonAccent/data_small\n",
        "# download\n",
        "from google.colab import files\n",
        "files.download(\"/content/accent-recog-slt2022/CommonAccent/data_small.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-PmRkAJCnmV",
        "outputId": "dd9d16e7-0c9d-4044-ba9c-7e05c7b4a731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accent\n",
            "TENDENTE AL SICULO MA NON MARCATO    1801\n",
            "BASILICATA TRENTINO                  1801\n",
            "VENETO                               1393\n",
            "MERIDIONALE                           134\n",
            "EMILIANO                               91\n",
            "Name: count, dtype: int64\n",
            "accent\n",
            "BASILICATA TRENTINO                  300\n",
            "TENDENTE AL SICULO MA NON MARCATO    300\n",
            "VENETO                               300\n",
            "Name: count, dtype: int64\n",
            "accent\n",
            "BASILICATA TRENTINO                  50\n",
            "TENDENTE AL SICULO MA NON MARCATO    50\n",
            "VENETO                               50\n",
            "Name: count, dtype: int64\n",
            "accent\n",
            "BASILICATA TRENTINO                  50\n",
            "TENDENTE AL SICULO MA NON MARCATO    50\n",
            "VENETO                               50\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-f3c15a7b6ddf>:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df_train2 = df_train.groupby('accent').apply(lambda x: x.sample(min(len(x), 300), random_state=1)).reset_index(drop=True)\n",
            "<ipython-input-20-f3c15a7b6ddf>:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df_dev2 = df_dev.groupby('accent').apply(lambda x: x.sample(min(len(x), 50), random_state=1)).reset_index(drop=True)\n",
            "<ipython-input-20-f3c15a7b6ddf>:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df_test2 = df_test.groupby('accent').apply(lambda x: x.sample(min(len(x), 50), random_state=1)).reset_index(drop=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save audios locally (takes around 27min for italian)\n",
        "for item1 in cv_19_train:\n",
        "    # Check if the current file path is in the CSV file list\n",
        "    # print(item['path'])\n",
        "    # adjust the following line to your working directory\n",
        "    filename = (item1['path'].split('/')[-2] + '/' + item1['path'].split('/')[-1])\n",
        "    if filename in file_list_train or filename in file_list_test or filename in file_list_dev:\n",
        "      # Decode audio\n",
        "      audio_array = item1['audio']['array']  # Audio waveform\n",
        "      sampling_rate = item1['audio']['sampling_rate']  # Sampling rate\n",
        "\n",
        "      # Create the filename\n",
        "      file_name = os.path.join(save_dir, filename)\n",
        "      os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "\n",
        "      # Save the audio as a WAV file\n",
        "      write(file_name, sampling_rate, audio_array.astype(np.float32))  # Save as float32 for WAV compatibility\n"
      ],
      "metadata": {
        "id": "qYFe1THtuHEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takes 2 min for it dev, test\n",
        "for item1 in cv_19_dev:\n",
        "    filename = (item1['path'].split('/')[-2] + '/' + item1['path'].split('/')[-1])\n",
        "    if filename in file_list_train or filename in file_list_test or filename in file_list_dev:\n",
        "      # Decode audio\n",
        "      audio_array = item1['audio']['array']  # Audio waveform\n",
        "      sampling_rate = item1['audio']['sampling_rate']  # Sampling rate\n",
        "\n",
        "      # Create the filename\n",
        "      file_name = os.path.join(save_dir, filename)\n",
        "      os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "\n",
        "      # Save the audio as a WAV file\n",
        "      write(file_name, sampling_rate, audio_array.astype(np.float32))  # Save as float32 for WAV compatibility"
      ],
      "metadata": {
        "id": "VtBeEGDtYCRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item1 in cv_19_test:\n",
        "    filename = (item1['path'].split('/')[-2] + '/' + item1['path'].split('/')[-1])\n",
        "    if filename in file_list_train or filename in file_list_test or filename in file_list_dev:\n",
        "      # Decode audio\n",
        "      audio_array = item1['audio']['array']  # Audio waveform\n",
        "      sampling_rate = item1['audio']['sampling_rate']  # Sampling rate\n",
        "\n",
        "      # Create the filename\n",
        "      file_name = os.path.join(save_dir, filename)\n",
        "      os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "\n",
        "      # Save the audio as a WAV file\n",
        "      write(file_name, sampling_rate, audio_array.astype(np.float32))  # Save as float32 for WAV compatibility"
      ],
      "metadata": {
        "id": "ZfyNah9KYIYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this step I would suggest to copy the .csv files and also the wav files to your gdrive so that you can load them from there next time.\n",
        "Next time (if you saved the wav files in your gdrive) you can copy it back to colab and can continue from there."
      ],
      "metadata": {
        "id": "-1n3cTWFZUVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir -p /content/accent-recog-slt2022/CommonAccent/data_small\n",
        "# !cp -R /content/drive/MyDrive/.../data/wav /content/accent-recog-slt2022/CommonAccent/data_small/wav_files"
      ],
      "metadata": {
        "id": "VGFp_4Bf_d_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -R /content/accent-recog-slt2022/CommonAccent/data_small/wav_files /content/drive/MyDrive/.../data/wav"
      ],
      "metadata": {
        "id": "IF5PqPbiXsdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "import librosa\n",
        "import os\n",
        "\n",
        "# resample all files to 16khz\n",
        "base_dir = '/content/accent-recog-slt2022/CommonAccent/data_small/wav_files/wav_files/'\n",
        "\n",
        "# Walk through all subdirectories\n",
        "for root, dirs, files in os.walk(base_dir):\n",
        "    for filename in files:\n",
        "        if filename.endswith('.mp3'):\n",
        "            audio_file = os.path.join(root, filename)\n",
        "            try:\n",
        "                # Load with librosa to ensure mono and 16kHz resampling\n",
        "                data, _ = librosa.load(audio_file, sr=16000, mono=True)\n",
        "                # Output path with .wav extension\n",
        "                new_path = audio_file.replace('.mp3', '.wav')\n",
        "                # Save as 16kHz mono .wav\n",
        "                sf.write(new_path, data, 16000)\n",
        "                # print(f\"Converted and saved: {new_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {audio_file}: {e}\")\n"
      ],
      "metadata": {
        "id": "2juSor7lxzis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies for training the model\n",
        "You should not get any errors here"
      ],
      "metadata": {
        "id": "fXxkRQoeD0bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pandas==2.2.2 \\\n",
        "    huggingface_hub>=0.7.0 \\\n",
        "    hyperpyyaml==0.0.1 \\\n",
        "    joblib \\\n",
        "    numpy==2.0.2 \\\n",
        "    packaging \\\n",
        "    pre-commit==2.3.0 \\\n",
        "    sentencepiece>=0.1.91 \\\n",
        "    SoundFile \\\n",
        "    tqdm"
      ],
      "metadata": {
        "id": "1oTznOs9vSkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga3ThIIw8q5K"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You will probably have to adjust your audiopaths in the csv files:"
      ],
      "metadata": {
        "id": "p_NN_MHhEAyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/accent-recog-slt2022/CommonAccent/data_small/train.csv /content/accent-recog-slt2022/CommonAccent/data_small/train_orig.csv\n",
        "!cp /content/accent-recog-slt2022/CommonAccent/data_small/dev.csv /content/accent-recog-slt2022/CommonAccent/data_small/dev_orig.csv\n",
        "!cp /content/accent-recog-slt2022/CommonAccent/data_small/test.csv /content/accent-recog-slt2022/CommonAccent/data_small/test_orig.csv"
      ],
      "metadata": {
        "id": "GwlLLx15DxV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "def update_csv_with_full_path_and_duration(\n",
        "    input_csv_path,\n",
        "    output_csv_path,\n",
        "    base_audio_path\n",
        "):\n",
        "    modified_rows = []\n",
        "\n",
        "    with open(input_csv_path, mode='r', newline='', encoding='utf-8') as infile:\n",
        "        reader = csv.DictReader(infile)\n",
        "\n",
        "        for row in reader:\n",
        "            try:\n",
        "                audio_path = os.path.join(base_audio_path, row['wav'].replace('.mp3', '.wav'))\n",
        "\n",
        "                data, sampling_rate = sf.read(audio_path)\n",
        "                duration = np.round(librosa.get_duration(y=data, sr=sampling_rate), 3)\n",
        "\n",
        "                row['duration'] = duration\n",
        "                row['wav'] = audio_path\n",
        "\n",
        "                modified_rows.append(row)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed processing {row['wav']}: {e}\")\n",
        "\n",
        "        fieldnames = reader.fieldnames\n",
        "\n",
        "    with open(output_csv_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(modified_rows)\n"
      ],
      "metadata": {
        "id": "PEaM7tT-NCRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_audio_path = '/content/accent-recog-slt2022/CommonAccent/data_small/wav_files/wav_files/'\n",
        "\n",
        "# Train\n",
        "update_csv_with_full_path_and_duration(\n",
        "    input_csv_path='/content/accent-recog-slt2022/CommonAccent/data_small/train_orig.csv',\n",
        "    output_csv_path='/content/accent-recog-slt2022/CommonAccent/data_small/train.csv',\n",
        "    base_audio_path=base_audio_path\n",
        ")\n",
        "\n",
        "# Dev\n",
        "update_csv_with_full_path_and_duration(\n",
        "    input_csv_path='/content/accent-recog-slt2022/CommonAccent/data_small/dev_orig.csv',\n",
        "    output_csv_path='/content/accent-recog-slt2022/CommonAccent/data_small/dev.csv',\n",
        "    base_audio_path=base_audio_path\n",
        ")\n",
        "\n",
        "# Test\n",
        "update_csv_with_full_path_and_duration(\n",
        "    input_csv_path='/content/accent-recog-slt2022/CommonAccent/data_small/test_orig.csv',\n",
        "    output_csv_path='/content/accent-recog-slt2022/CommonAccent/data_small/test.csv',\n",
        "    base_audio_path=base_audio_path\n",
        ")"
      ],
      "metadata": {
        "id": "NzNOJYwKNGWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/accent-recog-slt2022/CommonAccent\n",
        "# make .sh file executable:\n",
        "!chmod u+x run_accent_id_ecapa_tdnn.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS96svSH3Sti",
        "outputId": "5574daf0-8058-45b5-d46e-f3ef61be2d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/accent-recog-slt2022/CommonAccent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finally, we can run the model.\n",
        "You can try to run the training using cpu by adding\n",
        "\n",
        "--device=\"cpu\"\n",
        "\n",
        "to your .sh file (line 59)"
      ],
      "metadata": {
        "id": "3-LFB-nKEaHx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5Vz3bS0SBA4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e29693cc-d8d1-4bbe-a307-ddbd6eb27c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** About to start the training ***\n",
            "*** output folder: results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986 ***\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.pretrained.fetching - Fetch embedding_model.ckpt: Delegating to Huggingface hub, source speechbrain/spkrec-ecapa-voxceleb.\n",
            "speechbrain.utils.parameter_transfer - Loading pretrained files for: embedding_model\n",
            "speechbrain.core - Info: device arg overridden by command line input to: cuda:0\n",
            "speechbrain.core - 20.8M trainable parameters in AID\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "100% 29/29 [00:32<00:00,  1.13s/it, train_loss=7.51]\n",
            "100% 5/5 [00:02<00:00,  1.77it/s]\n",
            "speechbrain.utils.train_logger - Epoch: 1, lr: 1.00e-04 - train loss: 7.51 - valid loss: 4.81, valid error_rate: 2.67e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-41-42+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "100% 29/29 [00:31<00:00,  1.10s/it, train_loss=6.89]\n",
            "100% 5/5 [00:02<00:00,  2.48it/s]\n",
            "speechbrain.utils.train_logger - Epoch: 2, lr: 1.00e-04 - train loss: 6.89 - valid loss: 3.94, valid error_rate: 2.47e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-42-16+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-41-42+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100% 29/29 [00:33<00:00,  1.15s/it, train_loss=6.41]\n",
            "100% 5/5 [00:02<00:00,  2.38it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0001 to 9e-05\n",
            "speechbrain.utils.train_logger - Epoch: 3, lr: 1.00e-04 - train loss: 6.41 - valid loss: 3.44, valid error_rate: 3.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-42-52+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "100% 29/29 [00:34<00:00,  1.20s/it, train_loss=6.14]\n",
            "100% 5/5 [00:02<00:00,  1.95it/s]\n",
            "speechbrain.utils.train_logger - Epoch: 4, lr: 9.00e-05 - train loss: 6.14 - valid loss: 2.88, valid error_rate: 2.20e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-43-30+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-42-16+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-42-52+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "100% 29/29 [00:34<00:00,  1.18s/it, train_loss=5.85]\n",
            "100% 5/5 [00:02<00:00,  2.36it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-05 to 8.1e-05\n",
            "speechbrain.utils.train_logger - Epoch: 5, lr: 9.00e-05 - train loss: 5.85 - valid loss: 3.23, valid error_rate: 3.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-44-07+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-43-30+00\n",
            "100% 5/5 [00:02<00:00,  1.97it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 4 - test loss: 2.82, test error_rate: 1.60e-01\n",
            "Done training of speechbrain/spkrec-ecapa-voxceleb/embedding_model.ckpt in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986\n"
          ]
        }
      ],
      "source": [
        "!./run_accent_id_ecapa_tdnn.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the correct pretrained_path: /content/accent-recog-slt2022/CommonAccent/results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/\n",
        "# and load_folder: !ref <pretrained_path>/CKPT+2025-05-15+09-43-30+00\n",
        "# in the yaml file!\n",
        "# if run is successful, it will create an analysis folder (in your results folder)\n",
        "!python accent_id/inference.py accent_id/hparams/inference_ecapa_tdnn.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoByyoA_Rsls",
        "outputId": "15d01ebb-0300-4419-c3ad-9bc9902edcad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/analysis\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.pretrained.fetching - Fetch embedding_model.ckpt: Using existing file/symlink in model_checkpoints/embedding_model.ckpt.\n",
            "speechbrain.pretrained.fetching - Fetch classifier.ckpt: Using existing file/symlink in model_checkpoints/classifier.ckpt.\n",
            "speechbrain.pretrained.fetching - Fetch accent_encoder.txt: Using existing file/symlink in model_checkpoints/label_encoder.ckpt.\n",
            "speechbrain.utils.parameter_transfer - Loading pretrained files for: embedding_model, classifier, label_encoder\n",
            "speechbrain.core - Info: device arg from hparam file is used\n",
            "speechbrain.core - 20.8M trainable parameters in AccID_inf\n",
            "100% 5/5 [00:02<00:00,  1.88it/s]\n",
            "100% 5/5 [00:02<00:00,  2.47it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our project starts here!\n",
        "\n",
        "We have chosen the following dataset\n",
        "**https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0**\n",
        "\n",
        "for 3 languages:\n",
        "*   **Russian**\n",
        "*   **Belarusian**\n",
        "*   **Serbian**"
      ],
      "metadata": {
        "id": "-p1-S2E2D-ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Serbian"
      ],
      "metadata": {
        "id": "zHKM7Vx8IVXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/accent-recog-slt2022/CommonAccent/download_data_hf.py --language \"be\" /content/cv_data"
      ],
      "metadata": {
        "id": "QDMi0CqN7_zd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eddf2549-1cbc-49a7-c65f-03f22bfb27e6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/accent-recog-slt2022/CommonAccent/download_data_hf.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Russian"
      ],
      "metadata": {
        "id": "JrXWxfxoFDU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "tar_path = \"/content/cv-corpus-21.0-delta-2025-03-14-ru.tar\" #your downloaded locally dataset\n",
        "extract_path = \"/content/cv_data\"\n",
        "\n",
        "with tarfile.open(tar_path, \"r\") as tar:\n",
        "    tar.extractall(path=extract_path)\n",
        "\n",
        "print(\"Extraction complete.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbxykxcPRnuH",
        "outputId": "d5f133b3-dae1-4f0c-8830-cce825b921ad"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q speechbrain==0.5.13 \\\n",
        "    transformers==4.25.1 \\\n",
        "    librosa==0.9.2 \\\n",
        "    ipdb"
      ],
      "metadata": {
        "id": "1GaYRSeBS8Hv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "data_path = \"/content/cv_data/cv-corpus-21.0-delta-2025-03-14/ru\"\n",
        "\n",
        "validated = pd.read_csv(os.path.join(data_path, \"validated.tsv\"), sep='\\t')\n",
        "other = pd.read_csv(os.path.join(data_path, \"other.tsv\"), sep='\\t')\n",
        "\n",
        "print(f\"Validated samples: {len(validated)}\")\n",
        "print(f\"Other samples: {len(other)}\")\n",
        "\n",
        "df_full = pd.concat([validated, other], ignore_index=True)\n",
        "\n",
        "print(f\"Total samples combined: {len(df_full)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd_hd-3VYKiC",
        "outputId": "4320f1ca-6b3e-4b9d-bb14-68ce3bc37625"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validated samples: 90\n",
            "Other samples: 2940\n",
            "Total samples combined: 3030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "data_path = \"/content/cv_data/cv-corpus-21.0-delta-2025-03-14/ru\"\n",
        "validated_path = os.path.join(data_path, \"validated.tsv\")\n",
        "other_path = os.path.join(data_path, \"other.tsv\")\n",
        "\n",
        "df_validated = pd.read_csv(validated_path, sep='\\t')\n",
        "df_other = pd.read_csv(other_path, sep='\\t')\n",
        "\n",
        "df_validated['source'] = 'validated'\n",
        "df_other['source'] = 'other'\n",
        "\n",
        "df_combined = pd.concat([df_validated, df_other], ignore_index=True)\n",
        "\n",
        "print(f\"Validated samples: {len(df_validated)}\")\n",
        "print(f\"Other samples: {len(df_other)}\")\n",
        "print(f\"Total samples combined: {len(df_combined)}\")\n",
        "\n",
        "# 70% train, 15% dev, 15% test\n",
        "df_train_ru, df_temp_ru = train_test_split(df_combined, test_size=0.3, random_state=42)\n",
        "df_dev_ru, df_test_ru = train_test_split(df_temp_ru, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Train size russian: {len(df_train_ru)}\")\n",
        "print(f\"Dev size russian: {len(df_dev_ru)}\")\n",
        "print(f\"Test size russian: {len(df_test_ru)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAB8p6cggAhr",
        "outputId": "17a0586a-8ebb-43d4-f978-2b321b8be423"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validated samples: 90\n",
            "Other samples: 2940\n",
            "Total samples combined: 3030\n",
            "Train size russian: 2121\n",
            "Dev size russian: 454\n",
            "Test size russian: 455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Belarusian"
      ],
      "metadata": {
        "id": "bMcW-a2YFF_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loading the dataset locally\n",
        "\n",
        "Actually, you don't have to do this, as the dataset gets downloaded from the code below directly from HuggingFace. I have just realised it very late, don't waste your time trying to implement this :("
      ],
      "metadata": {
        "id": "NzvYzJGgRLus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive') #use this if you want to upload the file from colab"
      ],
      "metadata": {
        "id": "oZ19BTktL5Gm",
        "outputId": "ccb3aa3c-f9cc-4dba-c90c-f31c17ed99ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gdrive_path = \"/content/drive/MyDrive/Colab Notebooks/cv-corpus-14.0-delta-2023-06-23-be.tar.gz\" #change to your location on Google Drive - look it up in the directory from the left\n",
        "#extract_path = '/content/cv_data'\n",
        "#import tarfile\n",
        "\n",
        "#with tarfile.open(gdrive_path, \"r:gz\") as tar:\n",
        "#    tar.extractall(path=extract_path)\n",
        "\n",
        "#print(\"Extraction complete.\")"
      ],
      "metadata": {
        "id": "3RTQqKC7L9VW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing from the directory\n",
        "#import tarfile\n",
        "\n",
        "#tar_path = \"/content/cv-corpus-21.0-delta-2025-03-14-be.tar.gz\" #your downloaded locally dataset\n",
        "#extract_path = \"/content/cv_data\n",
        "\n",
        "#with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "#    tar.extractall(path=extract_path)\n",
        "\n",
        "#print(\"Extraction complete.\")"
      ],
      "metadata": {
        "id": "ZThrQxQmJwQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## processing the dataset"
      ],
      "metadata": {
        "id": "DXTmepMXROnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q speechbrain==0.5.13 \\\n",
        "    transformers==4.25.1 \\\n",
        "    librosa==0.9.2 \\\n",
        "    ipdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3yeTiUBMg0p",
        "outputId": "9eb5adc1-44c1-4728-88a0-34ff970e8199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "download_data_hf.py is a script in the loaded repo\n",
        "it is situated here: /content/accent-recog-slt2022/CommonAccent/download_data_hf.py\n",
        "\n",
        "**use the code from HugginFace and the number of the dataset you need, e.g.19  as in the script**\n",
        "https://huggingface.co/datasets/fsicoli/common_voice_19_0\n",
        "\n",
        "\n",
        "```\n",
        "\"\"\"\n",
        "Script to download the CommonVoice dataset using Hugging Face\n",
        "\n",
        "At Hugging Face: https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0\n",
        "Download the dataset: https://commonvoice.mozilla.org/en/datasets\n",
        "\n",
        "def prepare_cv_from_hf(output_folder, language=\"en\"):\n",
        "    \"\"\" function to prepare the datasets in <output-folder> \"\"\"\n",
        "\n",
        "    output_folder = os.path.join(output_folder, language)\n",
        "    # create the output folder: in case is not present\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Prepare the the common voice dataset in streaming mode\n",
        "    # common_voice_ds = load_dataset(_COMMON_VOICE_FOLDER, language, streaming=True)\n",
        "    common_voice_ds = load_dataset(\"fsicoli/common_voice_19_0\", language, streaming=True, trust_remote_code=True)\n",
        "\n",
        "    !!!!\n",
        "    it uses here already a dataset\n",
        "\n",
        "    # just select relevant splits: train/validation/test set\n",
        "    splits = [\"train\", \"validation\", \"test\"]\n",
        "    common_voice = {}\n",
        "    \n",
        "    # load, prepare and filter each split in streaming mode:\n",
        "    for split in splits:\n",
        "        # filter out samples without accent\n",
        "        ds = common_voice_ds[split].filter( lambda x: x['accent'] != '')\n",
        "        common_voice[split] = ds\n",
        "        \n",
        "    for dataset in common_voice:\n",
        "        csv_lines = []\n",
        "        # Starting index\n",
        "        idx = 0\n",
        "        for sample in common_voice[dataset]:\n",
        "            # get path and utt_id\n",
        "            mp3_path = sample['path']\n",
        "            utt_id = mp3_path.split(\".\")[-2].split(\"/\")[-1]            \n",
        "            \n",
        "            # Create a row with metadata + transcripts\n",
        "            csv_line = [\n",
        "                idx,  # ID\n",
        "                utt_id,  # Utterance ID\n",
        "                mp3_path,  # File name\n",
        "                sample[\"locale\"],\n",
        "                sample[\"accent\"],\n",
        "                sample[\"age\"],\n",
        "                sample[\"gender\"],\n",
        "                sample[\"sentence\"], # transcript\n",
        "            ]\n",
        "\n",
        "            # Adding this line to the csv_lines list\n",
        "            csv_lines.append(csv_line)\n",
        "            # Increment index\n",
        "            idx += 1\n",
        "\n",
        "        # CSV column titles\n",
        "        csv_header = [\"idx\", \"utt_id\", \"mp3_path\", \"language\", \"accent\", \"age\", \"gender\", \"transcript\"]\n",
        "        # Add titles to the list at indexx 0\n",
        "        csv_lines.insert(0, csv_header)\n",
        "\n",
        "        # Writing the csv lines\n",
        "        csv_file = os.path.join(output_folder, dataset+'.tsv')\n",
        "\n",
        "        with open(csv_file, mode=\"w\", encoding=\"utf-8\") as csv_f:\n",
        "            csv_writer = csv.writer(\n",
        "                csv_f, delimiter=\"\\t\", quotechar='\"', quoting=csv.QUOTE_MINIMAL\n",
        "            )\n",
        "            for line in csv_lines:\n",
        "                csv_writer.writerow(line)\n",
        "    print(f\"Prepare CommonVoice: for {language} in {output_folder}\")\n",
        "\n",
        "def main():\n",
        "    # read input from CLI, you need to run it from the command lind\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # reporting vars\n",
        "    parser.add_argument(\n",
        "        \"--language\",\n",
        "        type=str,\n",
        "        default='en',\n",
        "        help=\"Language to load\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"output_folder\",\n",
        "        help=\"path of the output folder to store the csv files for each split\",\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "tjn_F7-cRozj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So, the script loadss already one dataset\n",
        "    common_voice_ds = load_dataset(\"fsicoli/common_voice_19_0\", language, streaming=True, trust_remote_code=True), therefore you can run the cell below the same as it was in the original notebook.\n",
        "    If you need, however, abother dataset, e.g. wit hmore audio files, you need copz the code above and specify it and run the code without a script but your code in the cells**"
      ],
      "metadata": {
        "id": "DOnFOrZ99Slu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from datasets import load_dataset\n",
        "#common_voice_ds = load_dataset(\"mozilla-foundation/common_voice_19_0\", \"be\")"
      ],
      "metadata": {
        "id": "0snm1YFum2sP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/accent-recog-slt2022/CommonAccent/download_data_hf.py --language \"be\" /content/cv_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FTuIFlzm_Ue",
        "outputId": "7940c43e-538c-4cd5-dea7-7ac97be875a7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/accent-recog-slt2022/CommonAccent/download_data_hf.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "common_accent_prepare.py is a script in the loaded repo\n",
        "it is situated here: /content/accent-recog-slt2022/CommonAccent/common_accent_prepare.py\n",
        "\n",
        "```\n",
        "def prepare_common_accent(\n",
        "        data_folder,\n",
        "        save_folder,\n",
        "        accented_letters=False,\n",
        "        language=\"en\",        \n",
        "        skip_prep=False,\n",
        "    ):\n",
        "\n",
        "    \"\"\"\n",
        "    Prepares the csv files for the CommonAccent dataset for Accent Classification.\n",
        "    Download: https://commonvoice.mozilla.org/en/datasets\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    data_folder : str\n",
        "        Path to the folder where the CommonAccent dataset for Accent Classification is stored.\n",
        "        This path should include the multi: /datasets/CommonAccent\n",
        "    save_folder : str\n",
        "        The directory where to store the csv files.\n",
        "    max_duration : int, optional\n",
        "        Max duration (in seconds) of training uterances.\n",
        "    accented_letters : bool, optional\n",
        "        Defines if accented letters will be kept as individual letters or\n",
        "        transformed to the closest non-accented letters.\n",
        "    language: str\n",
        "        Specify the language for text normalization.        \n",
        "    skip_prep: bool\n",
        "        If True, skip data preparation.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> from recipes.CommonAccent.common_accent_prepare import prepare_common_accent\n",
        "    >>> data_folder = '/datasets/CommonAccent'\n",
        "    >>> save_folder = 'exp/CommonAccent_exp'\n",
        "    >>> prepare_common_accent(\\\n",
        "            data_folder,\\\n",
        "            save_folder,\\\n",
        "            skip_prep=False\\\n",
        "        )\n",
        "    \"\"\"\n",
        "```\n"
      ],
      "metadata": {
        "id": "4IY89c7CIqIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/accent-recog-slt2022\")\n",
        "\n",
        "from CommonAccent.common_accent_prepare import prepare_common_accent\n",
        "\n",
        "prepare_common_accent(\n",
        "    data_folder=\"/content/cv_data/be\",\n",
        "    save_folder=\"/content/common_accent_output\",\n",
        "    language=\"be\",\n",
        "    skip_prep=False\n",
        ")"
      ],
      "metadata": {
        "id": "F79x36GGn88O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python common_accent_prepare.py --language \"be\" /content/cv_data /content/common_accent_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxbTh_Vnh9_b",
        "outputId": "7668f949-504d-40c8-c185-4f8fd03f1c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/accent-recog-slt2022/CommonAccent/common_accent_prepare.py\", line 570, in <module>\n",
            "    main()\n",
            "  File \"/content/accent-recog-slt2022/CommonAccent/common_accent_prepare.py\", line 464, in main\n",
            "    prepare_common_accent(args.cv_folder, args.output_folder, language=args.language)\n",
            "  File \"/content/accent-recog-slt2022/CommonAccent/common_accent_prepare.py\", line 169, in prepare_common_accent\n",
            "    check_common_accent_folder(data_folder, language=language)\n",
            "  File \"/content/accent-recog-slt2022/CommonAccent/common_accent_prepare.py\", line 440, in check_common_accent_folder\n",
            "    raise FileNotFoundError(err_msg)\n",
            "FileNotFoundError: /content/cv_data/be must have at least 'train.tsv' file in it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import write\n",
        "from datasets import load_dataset\n",
        "\n",
        "data_path = \"/content/accent-recog-slt2022/CommonAccent/data/\"\n",
        "\n",
        "# Load CSV\n",
        "csv_path_train = data_path + \"train.csv\"\n",
        "df_train = pd.read_csv(csv_path_train)\n",
        "file_list_train = list(set(df_train['wav']))\n",
        "\n",
        "csv_path_test = data_path + \"test.csv\"\n",
        "df_test = pd.read_csv(csv_path_test)\n",
        "file_list_test = list(set(df_test['wav']))\n",
        "\n",
        "csv_path_dev = data_path + \"dev.csv\"\n",
        "df_dev = pd.read_csv(csv_path_dev)\n",
        "file_list_dev = list(set(df_dev['wav']))\n",
        "\n",
        "# Set save directory\n",
        "save_dir = data_path + \"wav_files\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Load dataset in streaming mode\n",
        "language = \"be\"  # or whatever language code you're using\n",
        "cv_11_be_train = load_dataset(\"mozilla-foundation/common_voice_11_0\", language, streaming=True, split=\"train\", trust_remote_code=True)\n",
        "print(next(iter(cv_11_be_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ad6a31e-3f22-402f-fd92-e8410005b469",
        "id": "J9dhLrqeFNvM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Reading metadata...: 171388it [00:07, 23662.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'client_id': 'c14f21cacf2b7701ad0bead0dd1b31ec9d3a1557708e446de55e98b4b470cf31072c82543e5ba518c5c187a91868878a4e32727054a3dd94f9df41c9a13d8c62', 'path': 'it_train_0/common_voice_it_17415777.mp3', 'audio': {'path': 'it_train_0/common_voice_it_17415777.mp3', 'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
            "       -1.61342832e-05, -1.57291943e-05, -1.16234423e-05]), 'sampling_rate': 48000}, 'sentence': \"Il marchese aveva già moglie in quell'epoca?\", 'up_votes': 3, 'down_votes': 0, 'age': '', 'gender': '', 'accent': '', 'locale': 'it', 'segment': '', 'variant': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_11_be_dev = load_dataset(\"mozilla-foundation/common_voice_11_0\", language, streaming=False, split=\"validation\", trust_remote_code=True)\n",
        "cv_11_be_test = load_dataset(\"mozilla-foundation/common_voice_11_0\", language, streaming=False, split=\"test\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "4Z2evoZ9FNvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Count entries per accent\n",
        "accent_counts = df_train['accent'].value_counts()\n",
        "\n",
        "# Print the result\n",
        "print(accent_counts)\n",
        "\n",
        "# select randomly max 300 samples for each accent that has more than 300 samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "003c3702-9ce5-40c1-8950-9693771089f4",
        "id": "myZLU5jiFNvM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accent\n",
            "TENDENTE AL SICULO MA NON MARCATO    1801\n",
            "BASILICATA TRENTINO                  1801\n",
            "VENETO                               1393\n",
            "MERIDIONALE                           134\n",
            "EMILIANO                               91\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## balancing"
      ],
      "metadata": {
        "id": "xRW333kiUKTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# add /content/accent-recog-slt2022/CommonAccent/data_small to each \"wav\" in the csv .../train.csv\n",
        "save_dir_colab = data_path\n",
        "# \"content/accent-recog-slt2022/CommonAccent/data_small/\"\n",
        "df_train['wav'] = df_train['wav'].apply(lambda x: os.path.join(save_dir_colab, x))\n",
        "df_test['wav'] = df_test['wav'].apply(lambda x: os.path.join(save_dir_colab, x))\n",
        "df_dev['wav'] = df_dev['wav'].apply(lambda x: os.path.join(save_dir_colab, x))\n",
        "df_train.to_csv(csv_path_train, index=False)\n",
        "\n",
        "# show all available accents\n",
        "accent_counts = df_train['accent'].value_counts()\n",
        "\n",
        "# Print the result\n",
        "print(accent_counts)\n",
        "\n",
        "# select randomly max 300 samples for each accent\n",
        "df_train2 = df_train.groupby('accent').apply(lambda x: x.sample(min(len(x), 300), random_state=1)).reset_index(drop=True)\n",
        "df_dev2 = df_dev.groupby('accent').apply(lambda x: x.sample(min(len(x), 50), random_state=1)).reset_index(drop=True)\n",
        "df_test2 = df_test.groupby('accent').apply(lambda x: x.sample(min(len(x), 50), random_state=1)).reset_index(drop=True)\n",
        "# remove accents with less than 300 samples\n",
        "df_train2 = df_train2.groupby('accent').filter(lambda x: len(x) >= 300)\n",
        "# get names of remaining accents\n",
        "remaining_accents = df_train2['accent'].unique()\n",
        "# filter dev and test sets to keep only the remaining accents\n",
        "df_dev2 = df_dev2[df_dev2['accent'].isin(remaining_accents)]\n",
        "df_test2 = df_test2[df_test2['accent'].isin(remaining_accents)]\n",
        "# reduce to 50 samples for dev and test sets\n",
        "\n",
        "# print stats of the new dataframe\n",
        "accent_counts2_train = df_train2['accent'].value_counts()\n",
        "accent_counts2_dev = df_dev2['accent'].value_counts()\n",
        "accent_counts2_test = df_test2['accent'].value_counts()\n",
        "# Print the result\n",
        "print(accent_counts2_train)\n",
        "print(accent_counts2_dev)\n",
        "print(accent_counts2_test)\n",
        "\n",
        "# save csv again with these samples\n",
        "os.makedirs('/content/accent-recog-slt2022/CommonAccent/data_small', exist_ok=True)\n",
        "df_train2.to_csv('/content/accent-recog-slt2022/CommonAccent/data_small/train.csv', index=False)\n",
        "df_dev2.to_csv('/content/accent-recog-slt2022/CommonAccent/data_small/dev.csv', index=False)\n",
        "df_test2.to_csv('/content/accent-recog-slt2022/CommonAccent/data_small/test.csv', index=False)\n",
        "# download files to disk\n",
        "!zip -r /content/accent-recog-slt2022/CommonAccent/data_small.zip /content/accent-recog-slt2022/CommonAccent/data_small\n",
        "# download\n",
        "from google.colab import files\n",
        "files.download(\"/content/accent-recog-slt2022/CommonAccent/data_small.zip\")"
      ],
      "metadata": {
        "id": "yWXG4Q16pnvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "data_path = \"...\"  # where the .wav files are\n",
        "csv_path_train = \"/content/common_accent_output/train.csv\"\n",
        "csv_path_dev = \"/content/common_accent_output/dev.csv\"\n",
        "csv_path_test = \"/content/common_accent_output/test.csv\"\n",
        "\n",
        "# Load CSVs\n",
        "df_train = pd.read_csv(csv_path_train, sep=\"\\t\")\n",
        "df_dev = pd.read_csv(csv_path_dev, sep=\"\\t\")\n",
        "df_test = pd.read_csv(csv_path_test, sep=\"\\t\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "P6zIS93NF3AU",
        "outputId": "841cabda-edc2-40e5-cb75-84bb5c6881cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/common_accent_output/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3206319271>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load CSVs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdf_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/common_accent_output/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add full path to each 'wav' file (clip name)\n",
        "df_train['wav'] = df_train['wav'].apply(lambda x: os.path.join(data_path, x))\n",
        "df_dev['wav'] = df_dev['wav'].apply(lambda x: os.path.join(data_path, x))\n",
        "df_test['wav'] = df_test['wav'].apply(lambda x: os.path.join(data_path, x))"
      ],
      "metadata": {
        "id": "KuFcq1plGsFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show all accents\n",
        "print(df_train['accent'].value_counts())\n",
        "\n",
        "# Sample & filter\n",
        "df_train2 = df_train.groupby('accent').apply(lambda x: x.sample(min(len(x), 300), random_state=1)).reset_index(drop=True)\n",
        "df_train2 = df_train2.groupby('accent').filter(lambda x: len(x) >= 300)\n",
        "\n",
        "remaining_accents = df_train2['accent'].unique()\n",
        "df_dev2 = df_dev[df_dev['accent'].isin(remaining_accents)].groupby('accent').apply(lambda x: x.sample(min(len(x), 50), random_state=1)).reset_index(drop=True)\n",
        "df_test2 = df_test[df_test['accent'].isin(remaining_accents)].groupby('accent').apply(lambda x: x.sample(min(len(x), 50), random_state=1)).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "DaeKr4TDHI2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save new CSVs\n",
        "save_path = \"/content/accent-recog-slt2022/CommonAccent/data_small\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "df_train2.to_csv(os.path.join(save_path, \"train.csv\"), index=False)\n",
        "df_dev2.to_csv(os.path.join(save_path, \"dev.csv\"), index=False)\n",
        "df_test2.to_csv(os.path.join(save_path, \"test.csv\"), index=False)\n",
        "\n",
        "# Zip and download\n",
        "!zip -r /content/data_small.zip {save_path}\n",
        "from google.colab import files\n",
        "files.download(\"/content/data_small.zip\")"
      ],
      "metadata": {
        "id": "x9wPw8SEHPfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save audios locally\n",
        "\n",
        "for item1 in cv_11_be_train:\n",
        "    # Check if the current file path is in the CSV file list\n",
        "    # print(item['path'])\n",
        "    # adjust the following line to your working directory\n",
        "    filename = (item1['path'].split('/')[-2] + '/' + item1['path'].split('/')[-1])\n",
        "    if filename in file_list_train or filename in file_list_test or filename in file_list_dev:\n",
        "      # Decode audio\n",
        "      audio_array = item1['audio']['array']  # Audio waveform\n",
        "      sampling_rate = item1['audio']['sampling_rate']  # Sampling rate\n",
        "\n",
        "      # Create the filename\n",
        "      file_name = os.path.join(save_dir, filename)\n",
        "      os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "\n",
        "      # Save the audio as a WAV file\n",
        "      write(file_name, sampling_rate, audio_array.astype(np.float32))  # Save as float32 for WAV compatibility"
      ],
      "metadata": {
        "id": "eMF0uESfULwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takes 2 min for it dev, test\n",
        "for item1 in cv_11_be_train:\n",
        "    filename = (item1['path'].split('/')[-2] + '/' + item1['path'].split('/')[-1])\n",
        "    if filename in file_list_train or filename in file_list_test or filename in file_list_dev:\n",
        "      # Decode audio\n",
        "      audio_array = item1['audio']['array']  # Audio waveform\n",
        "      sampling_rate = item1['audio']['sampling_rate']  # Sampling rate\n",
        "\n",
        "      # Create the filename\n",
        "      file_name = os.path.join(save_dir, filename)\n",
        "      os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "\n",
        "      # Save the audio as a WAV file\n",
        "      write(file_name, sampling_rate, audio_array.astype(np.float32))  # Save as float32 for WAV compatibility"
      ],
      "metadata": {
        "id": "x7a7BrjCULwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item1 in cv_11_be_train:\n",
        "    filename = (item1['path'].split('/')[-2] + '/' + item1['path'].split('/')[-1])\n",
        "    if filename in file_list_train or filename in file_list_test or filename in file_list_dev:\n",
        "      # Decode audio\n",
        "      audio_array = item1['audio']['array']  # Audio waveform\n",
        "      sampling_rate = item1['audio']['sampling_rate']  # Sampling rate\n",
        "\n",
        "      # Create the filename\n",
        "      file_name = os.path.join(save_dir, filename)\n",
        "      os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
        "\n",
        "      # Save the audio as a WAV file\n",
        "      write(file_name, sampling_rate, audio_array.astype(np.float32))  # Save as float32 for WAV compatibility"
      ],
      "metadata": {
        "id": "M88lqAWRULwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training"
      ],
      "metadata": {
        "id": "XyNqehXCT2wa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e29693cc-d8d1-4bbe-a307-ddbd6eb27c82",
        "id": "-ieiVxJUT7iF"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** About to start the training ***\n",
            "*** output folder: results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986 ***\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.pretrained.fetching - Fetch embedding_model.ckpt: Delegating to Huggingface hub, source speechbrain/spkrec-ecapa-voxceleb.\n",
            "speechbrain.utils.parameter_transfer - Loading pretrained files for: embedding_model\n",
            "speechbrain.core - Info: device arg overridden by command line input to: cuda:0\n",
            "speechbrain.core - 20.8M trainable parameters in AID\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "100% 29/29 [00:32<00:00,  1.13s/it, train_loss=7.51]\n",
            "100% 5/5 [00:02<00:00,  1.77it/s]\n",
            "speechbrain.utils.train_logger - Epoch: 1, lr: 1.00e-04 - train loss: 7.51 - valid loss: 4.81, valid error_rate: 2.67e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-41-42+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "100% 29/29 [00:31<00:00,  1.10s/it, train_loss=6.89]\n",
            "100% 5/5 [00:02<00:00,  2.48it/s]\n",
            "speechbrain.utils.train_logger - Epoch: 2, lr: 1.00e-04 - train loss: 6.89 - valid loss: 3.94, valid error_rate: 2.47e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-42-16+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-41-42+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100% 29/29 [00:33<00:00,  1.15s/it, train_loss=6.41]\n",
            "100% 5/5 [00:02<00:00,  2.38it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0001 to 9e-05\n",
            "speechbrain.utils.train_logger - Epoch: 3, lr: 1.00e-04 - train loss: 6.41 - valid loss: 3.44, valid error_rate: 3.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-42-52+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "100% 29/29 [00:34<00:00,  1.20s/it, train_loss=6.14]\n",
            "100% 5/5 [00:02<00:00,  1.95it/s]\n",
            "speechbrain.utils.train_logger - Epoch: 4, lr: 9.00e-05 - train loss: 6.14 - valid loss: 2.88, valid error_rate: 2.20e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-43-30+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-42-16+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-42-52+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "100% 29/29 [00:34<00:00,  1.18s/it, train_loss=5.85]\n",
            "100% 5/5 [00:02<00:00,  2.36it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-05 to 8.1e-05\n",
            "speechbrain.utils.train_logger - Epoch: 5, lr: 9.00e-05 - train loss: 5.85 - valid loss: 3.23, valid error_rate: 3.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-44-07+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/CKPT+2025-05-15+09-43-30+00\n",
            "100% 5/5 [00:02<00:00,  1.97it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 4 - test loss: 2.82, test error_rate: 1.60e-01\n",
            "Done training of speechbrain/spkrec-ecapa-voxceleb/embedding_model.ckpt in results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986\n"
          ]
        }
      ],
      "source": [
        "!./run_accent_id_ecapa_tdnn.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the correct pretrained_path: /content/accent-recog-slt2022/CommonAccent/results/ECAPA-TDNN/EN/spkrec-ecapa-voxceleb/1986/save/\n",
        "# and load_folder: !ref <pretrained_path>/CKPT+2025-05-15+09-43-30+00\n",
        "# in the yaml file!\n",
        "# if run is successful, it will create an analysis folder (in your results folder)\n",
        "!python accent_id/inference.py accent_id/hparams/inference_ecapa_tdnn.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d01ebb-0300-4419-c3ad-9bc9902edcad",
        "id": "Fr3KYxN2T7iF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/analysis\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.pretrained.fetching - Fetch embedding_model.ckpt: Using existing file/symlink in model_checkpoints/embedding_model.ckpt.\n",
            "speechbrain.pretrained.fetching - Fetch classifier.ckpt: Using existing file/symlink in model_checkpoints/classifier.ckpt.\n",
            "speechbrain.pretrained.fetching - Fetch accent_encoder.txt: Using existing file/symlink in model_checkpoints/label_encoder.ckpt.\n",
            "speechbrain.utils.parameter_transfer - Loading pretrained files for: embedding_model, classifier, label_encoder\n",
            "speechbrain.core - Info: device arg from hparam file is used\n",
            "speechbrain.core - 20.8M trainable parameters in AccID_inf\n",
            "100% 5/5 [00:02<00:00,  1.88it/s]\n",
            "100% 5/5 [00:02<00:00,  2.47it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluation with confusion matrix, error rate, precision, and F1-score"
      ],
      "metadata": {
        "id": "ED59kUemC3jW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## visualisation wit t-sne"
      ],
      "metadata": {
        "id": "SrJsf6iCUma6"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}